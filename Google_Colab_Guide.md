# üöÄ H∆Ø·ªöNG D·∫™N CH·∫†Y PROJECT TR√äN GOOGLE COLAB

## üìã T·ªïng quan
H∆∞·ªõng d·∫´n chi ti·∫øt ƒë·ªÉ ch·∫°y project **Telco Customer Churn Prediction** tr√™n Google Colab m·ªôt c√°ch ƒë∆°n gi·∫£n v√† hi·ªáu qu·∫£.

## ‚ö° ∆Øu ƒëi·ªÉm c·ªßa Google Colab
- ‚úÖ **Mi·ªÖn ph√≠** v√† c√≥ GPU/TPU
- ‚úÖ **Kh√¥ng c·∫ßn c√†i ƒë·∫∑t** g√¨ tr√™n m√°y t√≠nh
- ‚úÖ **ƒê√£ c√≥ s·∫µn** t·∫•t c·∫£ th∆∞ vi·ªán c·∫ßn thi·∫øt
- ‚úÖ **D·ªÖ chia s·∫ª** v√† collaborate
- ‚úÖ **L∆∞u tr·ªØ tr√™n Google Drive**

## üîß B∆∞·ªõc 1: Chu·∫©n b·ªã

### 1.1 T·∫°o Google Colab Notebook
1. Truy c·∫≠p [Google Colab](https://colab.research.google.com/)
2. ƒêƒÉng nh·∫≠p t√†i kho·∫£n Google
3. T·∫°o notebook m·ªõi: **File > New notebook**

### 1.2 Upload d·ªØ li·ªáu
```python
import pandas as pd
# N·∫øu c√≥ li√™n k·∫øt t·∫£i xu·ªëng tr·ª±c ti·∫øp
url = "https://raw.githubusercontent.com/tanmaiii/may_hoc_ung_dung/refs/heads/main/telco-customer-churn.csv"
df = pd.read_csv(url)
```

## üìä B∆∞·ªõc 2: Notebook ho√†n ch·ªânh

### 2.1 Setup v√† Import Libraries
```python
# C√†i ƒë·∫∑t th√™m g√≥i th∆∞ vi·ªán n·∫øu c·∫ßn
!pip install plotly seaborn

# Import c√°c th∆∞ vi·ªán
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# M√°y h·ªçc
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve

# T·∫Øt c·∫£nh b√°o
import warnings
warnings.filterwarnings('ignore')

# C√†i ƒë·∫∑t style cho bi·ªÉu ƒë·ªì
plt.style.use('default')
sns.set_palette("husl")
```

### 2.2 Load v√† Explore Data (Load d·ªØ li·ªáu)
```python
# T·∫£i d·ªØ li·ªáu
df = pd.read_csv('telco-customer-churn.csv')  # Ho·∫∑c ƒë∆∞·ªùng d·∫´n file b·∫°n upload

print("üéØ T·ªîNG QUAN D·ªÆ LI·ªÜU")
print(f"K√≠ch th∆∞·ªõc: {df.shape}")
print(f"Gi√° tr·ªã thi·∫øu: {df.isnull().sum().sum()}")
print("\nüìä 5 d√≤ng ƒë·∫ßu ti√™n:")
df.head()
```

### 2.3 Data Analysis v√† Visualization (Ph√¢n t√≠ch d·ªØ li·ªáu v√† tr·ª±c quan h√≥a)
```python
# Th√¥ng tin c∆° b·∫£n
print("üìà TH√îNG TIN D·ªÆ LI·ªÜU")
df.info()

print("\nüìä TH·ªêNG K√ä M√î T·∫¢")
df.describe()

# Ph√¢n b·ªë Churn
print("\nüéØ PH√ÇN B·ªê CHURN")
churn_counts = df['Churn'].value_counts()
print(churn_counts)
print(f"T·ª∑ l·ªá kh√°ch h√†ng r·ªùi b·ªè d·ªãch v·ª• (Churn rate): {churn_counts[1] / len(df) * 100:.2f}%")

# Tr·ª±c quan h√≥a
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# Bi·ªÉu ƒë·ªì tr√≤n
axes[0,0].pie(churn_counts.values, labels=churn_counts.index, autopct='%1.1f%%')
axes[0,0].set_title('T·ª∑ l·ªá kh√°ch h√†ng r·ªùi b·ªè (Churn)')

# Bi·ªÉu ƒë·ªì ph√¢n ph·ªëi
axes[0,1].hist(df['tenure'], bins=30, alpha=0.7)
axes[0,1].set_title('Ph√¢n b·ªë th·ªùi gian s·ª≠ d·ª•ng')
axes[0,1].set_xlabel('S·ªë th√°ng s·ª≠ d·ª•ng (tenure)')

# Bi·ªÉu ƒë·ªì h·ªôp
sns.boxplot(data=df, x='Churn', y='MonthlyCharges', ax=axes[1,0])
axes[1,0].set_title('Chi ph√≠ h√†ng th√°ng theo Churn')

# Bi·ªÉu ƒë·ªì c·ªôt
contract_churn = pd.crosstab(df['Contract'], df['Churn'])
contract_churn.plot(kind='bar', ax=axes[1,1])
axes[1,1].set_title('Lo·∫°i h·ª£p ƒë·ªìng v√† Churn')
axes[1,1].legend(title='Churn')

plt.tight_layout()
plt.show()
```

## üîÑ B∆∞·ªõc 3: Data Preprocessing (Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu)

### 3.1 Data Cleaning (L√†m s·∫°ch)
```python
# T·∫°o b·∫£n sao ƒë·ªÉ x·ª≠ l√Ω
df_clean = df.copy()

print("üßπ L√ÄM S·∫†CH D·ªÆ LI·ªÜU...")

# X·ª≠ l√Ω TotalCharges (c√≥ gi√° tr·ªã ' ')
print(f"Gi√° tr·ªã ' ' trong TotalCharges: {(df_clean['TotalCharges'] == ' ').sum()}")
df_clean['TotalCharges'] = df_clean['TotalCharges'].replace(' ', np.nan)
df_clean['TotalCharges'] = pd.to_numeric(df_clean['TotalCharges'], errors='coerce')

# ƒêi·ªÅn gi√° tr·ªã thi·∫øu
df_clean['TotalCharges'].fillna(0, inplace=True)

# Lo·∫°i b·ªè customerID
df_clean = df_clean.drop('customerID', axis=1)

print(f"‚úÖ K√≠ch th∆∞·ªõc sau l√†m s·∫°ch: {df_clean.shape}")
print(f"Gi√° tr·ªã thi·∫øu sau l√†m s·∫°ch: {df_clean.isnull().sum().sum()}")
```

### 3.2 Feature Engineering 
```python
print("üîß T·∫†O ƒê·∫∂C TR∆ØNG M·ªöI...")

# Ki·ªÉm tra c√°c gi√° tr·ªã unique trong t·∫•t c·∫£ c√°c c·ªôt object
print("üìä KI·ªÇM TRA C√ÅC GI√Å TR·ªä UNIQUE TRONG C√ÅC C·ªòT:")
for col in df_clean.columns:
    if df_clean[col].dtype == 'object':
        unique_vals = df_clean[col].unique()
        print(f"{col}: {unique_vals}")

# X·ª≠ l√Ω c√°c c·ªôt c√≥ gi√° tr·ªã ƒë·∫∑c bi·ªát (nh∆∞ "No phone service", "No internet service")
special_service_cols = ['MultipleLines', 'OnlineSecurity', 'OnlineBackup', 
                       'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies']

for col in special_service_cols:
    if col in df_clean.columns:
        print(f"\nüîß X·ª≠ l√Ω c·ªôt {col}...")
        # Chuy·ªÉn "No phone service" v√† "No internet service" th√†nh "No"
        df_clean[col] = df_clean[col].replace({'No phone service': 'No', 
                                             'No internet service': 'No'})
        print(f"Gi√° tr·ªã sau x·ª≠ l√Ω: {df_clean[col].unique()}")

# M√£ h√≥a nh·ªã ph√¢n cho c√°c c·ªôt Yes/No
binary_cols = []
for col in df_clean.columns:
    if df_clean[col].dtype == 'object' and col != 'Churn':
        unique_vals = df_clean[col].unique()
        if set(unique_vals).issubset({'Yes', 'No'}):
            binary_cols.append(col)
            df_clean[col] = df_clean[col].map({'Yes': 1, 'No': 0})

print(f"\n‚úÖ C√°c c·ªôt ƒë√£ m√£ h√≥a nh·ªã ph√¢n: {binary_cols}")

# M√£ h√≥a one-hot cho c√°c bi·∫øn ƒë·ªãnh danh
nominal_cols = ['gender', 'Contract', 'PaymentMethod', 'InternetService']
df_encoded = pd.get_dummies(df_clean, columns=nominal_cols, drop_first=True)

# Tr·∫£ v·ªÅ 1, 0 thay v√¨ yes, no
df_encoded['Churn'] = df_encoded['Churn'].map({'Yes': 1, 'No': 0})

# T·∫°o c√°c ƒë·∫∑c tr∆∞ng m·ªõi
df_encoded['AvgChargePerTenure'] = np.where(
    df_encoded['tenure'] > 0,
    df_encoded['TotalCharges'] / df_encoded['tenure'],
    df_encoded['MonthlyCharges']
)

# Nh√≥m th·ªùi gian s·ª≠ d·ª•ng - t·∫°o nh√≥m theo th·ªùi gian s·ª≠ d·ª•ng 
# 0: 0-12m (Kh√°ch h√†ng m·ªõi)
# 1: 13-36m (Kh√°ch h√†ng trung h·∫°n)
# 2: 37-60m (Kh√°ch h√†ng d√†i h·∫°n)
# 3: 60m+ (Kh√°ch h√†ng trung th√†nh)

try:
    df_encoded['TenureGroup'] = pd.cut(
        df_encoded['tenure'],
        bins=[0, 12, 36, 60, float('inf')],
        labels=['0-12m', '13-36m', '37-60m', '60m+']
    )
    # M√£ h√≥a nh√£n ph√¢n lo·∫°i th√†nh s·ªë
    df_encoded['TenureGroup'] = df_encoded['TenureGroup'].cat.codes
except Exception as e:
    print(f"L·ªói khi t·∫°o TenureGroup: {e}")
    # Ph∆∞∆°ng √°n d·ª± ph√≤ng: t·∫°o nh√≥m ƒë∆°n gi·∫£n
    df_encoded['TenureGroup'] = pd.cut(
        df_encoded['tenure'],
        bins=4,
        labels=[0, 1, 2, 3]
    ).astype(int)

# Ki·ªÉm tra c√°c c·ªôt c√≤n l·∫°i ch∆∞a ƒë∆∞·ª£c m√£ h√≥a
print(f"\nüìã KI·ªÇM TRA C√ÅC C·ªòT SAU KHI M√É H√ìA:")
remaining_object_cols = [col for col in df_encoded.columns if df_encoded[col].dtype == 'object']
if remaining_object_cols:
    print(f"‚ö†Ô∏è  C√°c c·ªôt ch∆∞a ƒë∆∞·ª£c m√£ h√≥a: {remaining_object_cols}")
    for col in remaining_object_cols:
        print(f"   {col}: {df_encoded[col].unique()}")
else:
    print("‚úÖ T·∫•t c·∫£ c√°c c·ªôt ƒë√£ ƒë∆∞·ª£c m√£ h√≥a th√†nh s·ªë!")

print(f"\n‚úÖ K√≠ch th∆∞·ªõc cu·ªëi c√πng: {df_encoded.shape}")
df_encoded.head()
```

## üéØ B∆∞·ªõc 4: Feature Selection

### 4.1 Correlation Analysis
```python
# B·∫£n ƒë·ªì nhi·ªát t∆∞∆°ng quan
plt.figure(figsize=(20, 16))
correlation_matrix = df_encoded.corr()
sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', center=0)
plt.title('Ma tr·∫≠n t∆∞∆°ng quan ƒë·∫∑c tr∆∞ng')
plt.show()

# T∆∞∆°ng quan cao nh·∫•t v·ªõi m·ª•c ti√™u
target_corr = correlation_matrix['Churn'].abs().sort_values(ascending=False)
print("üéØ TOP 15 ƒê·∫∂C TR∆ØNG T∆Ø∆†NG QUAN V·ªöI CHURN:")
print(target_corr.head(15))
```

### 4.2 Feature Selection Methods
```python
# Chu·∫©n b·ªã d·ªØ li·ªáu
X = df_encoded.drop('Churn', axis=1)
y = df_encoded['Churn']

print(f"K√≠ch th∆∞·ªõc ƒë·∫∑c tr∆∞ng: {X.shape}")
print(f"Ph√¢n b·ªë target: {y.value_counts().to_dict()}")

# Ph∆∞∆°ng ph√°p 1: SelectKBest
def select_features_univariate(X, y, k=10):
    selector = SelectKBest(score_func=f_classif, k=k)
    X_selected = selector.fit_transform(X, y)
    selected_features = X.columns[selector.get_support()].tolist()
    scores = selector.scores_
    
    feature_scores = pd.DataFrame({
        'feature': X.columns,
        'score': scores
    }).sort_values('score', ascending=False)
    
    return selected_features, feature_scores

# Ph∆∞∆°ng ph√°p 2: Random Forest Feature Importance
def select_features_rf(X, y, k=10):
    rf = RandomForestClassifier(n_estimators=100, random_state=42)
    rf.fit(X, y)
    
    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': rf.feature_importances_
    }).sort_values('importance', ascending=False)
    
    selected_features = feature_importance.head(k)['feature'].tolist()
    return selected_features, feature_importance

# √Åp d·ª•ng l·ª±a ch·ªçn ƒë·∫∑c tr∆∞ng
for k in [5, 10, 15]:
    print(f"\nüîç TOP {k} ƒê·∫∂C TR∆ØNG:")
    
    # L·ª±a ch·ªçn ƒë∆°n bi·∫øn
    features_uni, scores_uni = select_features_univariate(X, y, k)
    print(f"Ph∆∞∆°ng ph√°p Univariate: {features_uni}")
    
    # Random Forest
    features_rf, scores_rf = select_features_rf(X, y, k)
    print(f"Random Forest: {features_rf}")
```

## ü§ñ B∆∞·ªõc 5: Model Training v√† Evaluation

### 5.1 Data Splitting v√† Scaling
```python
# Chia d·ªØ li·ªáu
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Chu·∫©n h√≥a
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"T·∫≠p hu·∫•n luy·ªán: {X_train_scaled.shape}")
print(f"T·∫≠p ki·ªÉm tra: {X_test_scaled.shape}")
print(f"Ph√¢n b·ªë target trong t·∫≠p train: {y_train.value_counts(normalize=True).round(3).to_dict()}")
```

### 5.2 Model Training
```python
# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt cho thu·∫≠t to√°n
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.model_selection import cross_val_score

class MutilModel:
    def __init__(self):
        self.models = {
            # H·ªìi quy tuy·∫øn t√≠nh
            'LogisticRegression': LogisticRegression(
                solver='liblinear',   # t·ªët cho d·ªØ li·ªáu nh·ªè, ph√¢n lo·∫°i nh·ªã ph√¢n
                C=1.0,                # ƒë·ªô ph·∫°t (regularization) - nh·ªè h∆°n ‚Üí ch·ªëng overfit
                random_state=42
            ),
            # R·ª´ng ng·∫´u nhi√™n
            'RandomForest': RandomForestClassifier(
                n_estimators=200,     # s·ªë l∆∞·ª£ng c√¢y quy·∫øt ƒë·ªãnh
                max_depth=10,         # gi·ªõi h·∫°n ƒë·ªô s√¢u c√¢y ƒë·ªÉ tr√°nh overfitting
                min_samples_split=5,  # s·ªë m·∫´u √≠t nh·∫•t c·ªßa 1 n√∫t
                random_state=42
            ),
            # M√°y vector h·ªï tr·ª£ (SVC l√† bi·∫øn th·ªÉ c·ªßa SVM)
            'SVM': SVC(
                kernel='rbf',          # kernel ph·ªï bi·∫øn nh·∫•t
                C=1.0,                 # penalty, ƒëi·ªÅu ch·ªânh bi√™n ƒë·ªô margin
                gamma='scale',         # t·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh theo s·ªë chi·ªÅu
                probability=True,      # T√≠nh x√°c xu·∫•t d·ª± ƒëo√°n
                random_state=42
            ),
            # L√°ng gi·ªÅng g·∫ßn nh·∫•t
            'KNN': KNeighborsClassifier(
                n_neighbors=7,         # ch·ªçn s·ªë l√¢n c·∫≠n c·∫ßdn x√©t
                weights='distance',    # l√¢n c·∫≠n g·∫ßn h∆°n c√≥ tr·ªçng s·ªë l·ªõn h∆°n
                metric='minkowski'     # Kho·∫£ng c√°ch Minkowski
            ),
            # M·∫°ng n∆°ron
           'NeuralNetwork': MLPClassifier(
              hidden_layer_sizes=(128, 64, 32),  # tƒÉng s·ªë t·∫ßng ·∫©n, gi·∫£m d·∫ßn s·ªë neuron
              activation='relu',                # relu v·∫´n l√† t·ªët nh·∫•t v·ªõi d·ªØ li·ªáu phi tuy·∫øn
              solver='adam',                    # ·ªïn ƒë·ªãnh v√† nhanh
              alpha=0.0005,                     # h·ªá s·ªë regularization (L2), ch·ªëng overfitting
              learning_rate='adaptive',        # gi·∫£m learning rate khi g·∫∑p kh√≥
              learning_rate_init=0.001,        # learning rate kh·ªüi t·∫°o
              early_stopping=True,             # d·ª´ng s·ªõm n·∫øu kh√¥ng c·∫£i thi·ªán
              validation_fraction=0.1,         # 10% d·ªØ li·ªáu ƒë·ªÉ validation khi training
              max_iter=500,                    # s·ªë v√≤ng l·∫∑p (th∆∞·ªùng kh√¥ng c·∫ßn qu√° l·ªõn n·∫øu early stopping)
              random_state=42
          )
        }

    def train(self, X_train, y_train):
        for model in self.models.values():
            model.fit(X_train, y_train)

    def evaluation(self, X_test, y_test, X_train=None, y_train=None, cv=5):
        report = {}

        for name, model in self.models.items():
            y_pred = model.predict(X_test)
            y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None
            auc = roc_auc_score(y_test, y_proba) if y_proba is not None else None

            if X_train is not None and y_train is not None:
                cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')
                cv_mean = cv_scores.mean()
                cv_std = cv_scores.std()
            else:
                cv_mean = None
                cv_std = None

            report[name] = {
                'accuracy': accuracy_score(y_test, y_pred), # ƒê·ªô ch√≠nh x√°c
                'precision': precision_score(y_test, y_pred), # ƒê·ªô ch√≠nh x√°c theo d·ª± ƒëo√°n
                'recall': recall_score(y_test, y_pred), # Kh·∫£ nƒÉng ph√°t hi·ªán ƒë√∫ng
                'f1_score': f1_score(y_test, y_pred), # Ch·ªâ s·ªë c√¢n b·∫±ng gi·ªØa Precision v√† Recall
                'auc': auc, # kh·∫£ nƒÉng m√¥ h√¨nh ph√¢n bi·ªát
                'cv_mean': cv_mean,
                'cv_std': cv_std
            }

        return report

    def predict_customer(self, model_name, customer_data, reference_columns, scaler):
        model = self.models.get(model_name)
        if not model:
            raise ValueError(f"Model '{model_name}' not found.")

        # Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·∫ßu v√†o
        df_input = prepare_input(customer_data, reference_columns)
        df_scaled = scaler.transform(df_input)

        prediction = model.predict(df_scaled)[0]
        probability = model.predict_proba(df_scaled)[0][1] if hasattr(model, "predict_proba") else None

        return {
            "prediction": prediction,
            "probability": probability
        }

    def predict_all_customers(self, customer_data, reference_columns, scaler):
        # Chu·∫©n b·ªã DataFrame ƒë·∫ßu v√†o chung
        df_input = prepare_input(customer_data, reference_columns)
        df_scaled = scaler.transform(df_input)

        print("------------------------------üöÄüöÄüöÄ-----------------------------")

        # L·∫∑p qua t·∫•t c·∫£ m√¥ h√¨nh
        for name, model in self.models.items():
            pred = model.predict(df_scaled)[0]
            proba = model.predict_proba(df_scaled)[0][1] if hasattr(model, "predict_proba") else None

            # In k·∫øt qu·∫£
            print(f"\nüîç K·∫øt qu·∫£ c·ªßa m√¥ h√¨nh {name}:")
            print(f"-> {'üõ°Ô∏è ·ªû l·∫°i' if pred == 0 else 'üö∂‚Äç‚û°Ô∏è R·ªùi ƒëi'}")
            if proba is not None:
                print(f"  - X√°c su·∫•t churn: {proba:.4f}")
            else:
                print("  - X√°c su·∫•t churn: Kh√¥ng c√≥ (model kh√¥ng h·ªó tr·ª£ predict_proba)")

    def predict_customer(self, model_name, customer_data, reference_columns, scaler):
        model = self.models.get(model_name)
        if not model:
            raise ValueError(f"Model '{model_name}' not found.")

        # Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·∫ßu v√†o
        df_input = prepare_input(customer_data, reference_columns)
        df_scaled = scaler.transform(df_input)

        prediction = model.predict(df_scaled)[0]
        probability = model.predict_proba(df_scaled)[0][1] if hasattr(model, "predict_proba") else None

        return {
            "prediction": prediction,
            "probability": probability
        }

    def predict_all_customers(self, customer_data, reference_columns, scaler):
        """
        D·ª± ƒëo√°n cho c√πng 1 b·ªô d·ªØ li·ªáu ƒë·∫ßu v√†o (customer_data) v·ªõi t·∫•t c·∫£ c√°c model ƒë√£ l∆∞u.
        In ra k·∫øt qu·∫£ prediction v√† probability (n·∫øu c√≥) c·ªßa t·ª´ng model.
        """
        # Chu·∫©n b·ªã DataFrame ƒë·∫ßu v√†o chung
        df_input = prepare_input(customer_data, reference_columns)
        df_scaled = scaler.transform(df_input)

        print("------------------------------üöÄüöÄüöÄ-----------------------------")

        # L·∫∑p qua t·∫•t c·∫£ m√¥ h√¨nh
        for name, model in self.models.items():
            pred = model.predict(df_scaled)[0]
            proba = model.predict_proba(df_scaled)[0][1] if hasattr(model, "predict_proba") else None

            # In k·∫øt qu·∫£
            print(f"\nüîç K·∫øt qu·∫£ c·ªßa m√¥ h√¨nh {name}:")
            print(f"-> {'üõ°Ô∏è ·ªû l·∫°i' if pred == 0 else 'üö∂‚Äç‚û°Ô∏è R·ªùi ƒëi'}")
            if proba is not None:
                print(f"  - X√°c su·∫•t churn: {proba:.4f}")
            else:
                print("  - X√°c su·∫•t churn: Kh√¥ng c√≥ (model kh√¥ng h·ªó tr·ª£ predict_proba)")
```

#### Ch·∫°y model

```python
# Kh·ªüi t·∫°o v√† s·ª≠ d·ª•ng MutilModel
print("ü§ñ KH·ªûI T·∫†O V√Ä HU·∫§N LUY·ªÜN C√ÅC M√î H√åNH:")
multi_model = MutilModel()

print("üìã C√°c m√¥ h√¨nh ƒë∆∞·ª£c s·ª≠ d·ª•ng:")
for name in multi_model.models.keys():
    print(f"   - {name}")

# Hu·∫•n luy·ªán t·∫•t c·∫£ c√°c m√¥ h√¨nh
print(f"\nüîÑ ƒêang hu·∫•n luy·ªán {len(multi_model.models)} m√¥ h√¨nh...")
multi_model.train(X_train_scaled, y_train)
print("‚úÖ Ho√†n th√†nh hu·∫•n luy·ªán!")

# ƒê√°nh gi√° c√°c m√¥ h√¨nh
print(f"\nüìä ƒê√ÅNH GI√Å HI·ªÜU SU·∫§T:")
results = multi_model.evaluation(X_test_scaled, y_test, X_train_scaled, y_train)

# In k·∫øt qu·∫£ chi ti·∫øt
for name, metrics in results.items():
    print(f"\nüîπ {name}:")
    print(f"   Accuracy:  {metrics['accuracy']:.4f}") 
    print(f"   Precision: {metrics['precision']:.4f}")
    print(f"   Recall:    {metrics['recall']:.4f}")
    print(f"   F1-Score:  {metrics['f1_score']:.4f}")
    if metrics['auc'] is not None:
        print(f"   AUC:       {metrics['auc']:.4f}")
    if metrics['cv_mean'] is not None:
        print(f"   CV Score:  {metrics['cv_mean']:.4f} (¬±{metrics['cv_std']:.4f})")
```

```
ü§ñ KH·ªûI T·∫†O V√Ä HU·∫§N LUY·ªÜN C√ÅC M√î H√åNH:
üìã C√°c m√¥ h√¨nh ƒë∆∞·ª£c s·ª≠ d·ª•ng:
   - LogisticRegression
   - RandomForest
   - SVM
   - KNN
   - NeuralNetwork

üîÑ ƒêang hu·∫•n luy·ªán 5 m√¥ h√¨nh...
‚úÖ Ho√†n th√†nh hu·∫•n luy·ªán!

üìä ƒê√ÅNH GI√Å HI·ªÜU SU·∫§T:

üîπ LogisticRegression:
   Accuracy:  0.8197
   Precision: 0.6820
   Recall:    0.5979
   F1-Score:  0.6371
   AUC:       0.8626
   CV Score:  0.8005 (¬±0.0078)

üîπ RandomForest:
   Accuracy:  0.8148
   Precision: 0.7074
   Recall:    0.5121
   F1-Score:  0.5941
   AUC:       0.8615
   CV Score:  0.7984 (¬±0.0108)

üîπ SVM:
   Accuracy:  0.8112
   Precision: 0.6864
   Recall:    0.5282
   F1-Score:  0.5970
   AUC:       0.8214
   CV Score:  0.7966 (¬±0.0046)

üîπ KNN:
   Accuracy:  0.7779
   Precision: 0.5915
   Recall:    0.5201
   F1-Score:  0.5535
   AUC:       0.7941
   CV Score:  0.7528 (¬±0.0057)

üîπ NeuralNetwork:
   Accuracy:  0.8226
   Precision: 0.7030
   Recall:    0.5710
   F1-Score:  0.6302
   AUC:       0.8575
   CV Score:  0.7939 (¬±0.0052)
```

```python
# H√†m h·ªó tr·ª£: ƒë·∫£m b·∫£o customer_data c√≥ ƒë·ªß c·ªôt v√† ƒë√∫ng th·ª© t·ª±
def prepare_input(customer_data, reference_columns):
    df_input = pd.DataFrame([customer_data])

    # Th√™m c·ªôt thi·∫øu v·ªõi gi√° tr·ªã 0
    for col in reference_columns:
        if col not in df_input.columns:
            df_input[col] = 0

    # S·∫Øp x·∫øp theo ƒë√∫ng th·ª© t·ª± columns
    df_input = df_input[reference_columns]

    return df_input
```

```python
# Trung th√†nh, kh·∫£ nƒÉng ·ªü l·∫°i cao
customer_1 = {
   'gender_Female': 0,
    'SeniorCitizen': 0,
    'Partner_Yes': 1,
    'tenure': 50,
    'MonthlyCharges': 65.0,
    'InternetService_DSL': 1,
    'Contract_Two year': 1,
    'PaymentMethod_Bank transfer (automatic)': 1
}

# L·ªõn tu·ªïi, s·ªëng m·ªôt m√¨nh, d·ªãch v·ª• ƒë·∫Øt 
customer_2 = {
    'gender_Female': 0,
    'SeniorCitizen': 1,
    'Partner_Yes': 0,
    'tenure': 2,
    'MonthlyCharges': 95.5,
    'InternetService_Fiber optic': 1,
    'Contract_Month-to-month': 1,
    'PaymentMethod_Mailed check': 1
}

# M·ªõi, chi ph√≠ cao, h·ª£p ƒë·ªìng ng·∫Øn h·∫°n
customer_3 = {
    'gender_Female': 1,
    'SeniorCitizen': 1,
    'Partner_Yes': 0,
    'tenure': 1,
    'MonthlyCharges': 99.0,
    'InternetService_Fiber optic': 1,
    'Contract_Month-to-month': 1,
    'PaymentMethod_Electronic check': 1
}


multi_model.predict_all_customers(customer_1, X_train.columns, scaler)
multi_model.predict_all_customers(customer_2, X_train.columns, scaler)
multi_model.predict_all_customers(customer_3, X_train.columns, scaler)
```
## K·∫øt qu·∫£
### ------------------------------üöÄüöÄüöÄ-----------------------------

üîç K·∫øt qu·∫£ c·ªßa m√¥ h√¨nh LogisticRegression:
- -> üõ°Ô∏è ·ªû l·∫°i
- X√°c su·∫•t churn: 0.0074

üîç K·∫øt qu·∫£ c·ªßa m√¥ h√¨nh RandomForest:
- -> üõ°Ô∏è ·ªû l·∫°i
- X√°c su·∫•t churn: 0.2260

üîç K·∫øt qu·∫£ c·ªßa m√¥ h√¨nh SVM:
- -> üõ°Ô∏è ·ªû l·∫°i
- X√°c su·∫•t churn: 0.1222

üîç K·∫øt qu·∫£ c·ªßa m√¥ h√¨nh KNN:
- -> üõ°Ô∏è ·ªû l·∫°i
- X√°c su·∫•t churn: 0.2569

üîç K·∫øt qu·∫£ c·ªßa m√¥ h√¨nh NeuralNetwork:
- -> üõ°Ô∏è ·ªû l·∫°i
- X√°c su·∫•t churn: 0.0000
### ------------------------------üöÄüöÄüöÄ-----------------------------

üîç K·∫øt qu·∫£ c·ªßa m√¥ h√¨nh LogisticRegression:
- -> üö∂‚Äç‚û°Ô∏è R·ªùi ƒëi
- Xc su·∫•t churn: 0.5162

üîç K·∫øt qu·∫£ c·ªßa m√¥ h√¨nh RandomForest:
- -> üö∂‚Äç‚û°Ô∏è R·ªùi ƒëi
- X√°c su·∫•t churn: 0.6392

üîç K·∫øt qu·∫£ c·ªßa m√¥ h√¨nh SVM:
- -> üö∂‚Äç‚û°Ô∏è R·ªùi ƒëi
- X√°c su·∫•t churn: 0.6521

üîç K·∫øt qu·∫£ c·ªßa m√¥ h√¨nh KNN:
- -> üö∂‚Äç‚û°Ô∏è R·ªùi ƒëi
- X√°c su·∫•t churn: 0.8648

üîç K·∫øt qu·∫£ c·ªßa m√¥ h√¨nh NeuralNetwork:
- -> üõ°Ô∏è ·ªû l·∫°i
- X√°c su·∫•t churn: 0.0122

### 5.2.1 Linear Regression Analysis (Lecture 3)
```python
# Import th√™m cho Linear Regression v√† Clustering
from sklearn.linear_model import LinearRegression
from sklearn.cluster import KMeans
from sklearn.metrics import r2_score, mean_squared_error, silhouette_score

# Lecture 3: H·ªìi quy tuy·∫øn t√≠nh - Ph√¢n t√≠ch m·ªëi quan h·ªá
print("\nüìà PH√ÇN T√çCH H·ªíI QUY TUY·∫æN T√çNH:")

# H·ªìi quy MonthlyCharges d·ª±a tr√™n tenure
lr_monthly = LinearRegression()
X_tenure = df_encoded[['tenure']].values
y_monthly = df_encoded['MonthlyCharges'].values

lr_monthly.fit(X_tenure, y_monthly)
y_pred_monthly = lr_monthly.predict(X_tenure)

r2_monthly = r2_score(y_monthly, y_pred_monthly)
mse_monthly = mean_squared_error(y_monthly, y_pred_monthly)

print(f"H·ªìi quy MonthlyCharges ~ tenure:")
print(f"R¬≤: {r2_monthly:.4f}")
print(f"MSE: {mse_monthly:.4f}")
print(f"H·ªá s·ªë h·ªìi quy: {lr_monthly.coef_[0]:.4f}")
print(f"Intercept: {lr_monthly.intercept_:.4f}")

# V·∫Ω bi·ªÉu ƒë·ªì h·ªìi quy
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.scatter(X_tenure, y_monthly, alpha=0.5, label='D·ªØ li·ªáu th·ª±c')
plt.plot(X_tenure, y_pred_monthly, 'r-', label='ƒê∆∞·ªùng h·ªìi quy')
plt.xlabel('Tenure (th√°ng)')
plt.ylabel('Monthly Charges')
plt.title(f'H·ªìi quy tuy·∫øn t√≠nh\nR¬≤ = {r2_monthly:.4f}')
plt.legend()

# H·ªìi quy TotalCharges d·ª±a tr√™n tenure + MonthlyCharges
lr_total = LinearRegression()
X_multi = df_encoded[['tenure', 'MonthlyCharges']].values
y_total = df_encoded['TotalCharges'].values

lr_total.fit(X_multi, y_total)
y_pred_total = lr_total.predict(X_multi)
r2_total = r2_score(y_total, y_pred_total)

print(f"\nH·ªìi quy TotalCharges ~ tenure + MonthlyCharges:")
print(f"R¬≤: {r2_total:.4f}")
print(f"H·ªá s·ªë h·ªìi quy: {lr_total.coef_}")

plt.subplot(1, 2, 2)
plt.scatter(y_total, y_pred_total, alpha=0.5)
plt.plot([y_total.min(), y_total.max()], [y_total.min(), y_total.max()], 'r--')
plt.xlabel('TotalCharges th·ª±c t·∫ø')
plt.ylabel('TotalCharges d·ª± ƒëo√°n')
plt.title(f'D·ª± ƒëo√°n vs Th·ª±c t·∫ø\nR¬≤ = {r2_total:.4f}')

plt.tight_layout()
plt.show()
```

### 5.2.2 Clustering Analysis (Lecture 4+5)
```python
# Lecture 4+5: Ph√¢n c·ª•m kh√°ch h√†ng
print("\nüéØ PH√ÇN T√çCH PH√ÇN C·ª§M KH√ÅCH H√ÄNG:")

# Chu·∫©n b·ªã d·ªØ li·ªáu cho clustering (ch·ªâ d√πng numerical features)
cluster_features = ['tenure', 'MonthlyCharges', 'TotalCharges']
X_cluster = df_encoded[cluster_features].values
X_cluster_scaled = StandardScaler().fit_transform(X_cluster)

# T√¨m s·ªë cluster t·ªëi ∆∞u b·∫±ng Elbow method
inertias = []
silhouette_scores = []
K_range = range(2, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_cluster_scaled)
    inertias.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(X_cluster_scaled, kmeans.labels_))

# V·∫Ω bi·ªÉu ƒë·ªì Elbow v√† Silhouette
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# Elbow Method
axes[0].plot(K_range, inertias, 'bo-')
axes[0].set_xlabel('S·ªë c·ª•m (K)')
axes[0].set_ylabel('Inertia')
axes[0].set_title('Ph∆∞∆°ng ph√°p Elbow')
axes[0].grid(True)

# Silhouette Score
axes[1].plot(K_range, silhouette_scores, 'ro-')
axes[1].set_xlabel('S·ªë c·ª•m (K)')
axes[1].set_ylabel('Silhouette Score')
axes[1].set_title('ƒêi·ªÉm Silhouette')
axes[1].grid(True)

# Ch·ªçn K t·ªëi ∆∞u (K=3 ho·∫∑c K c√≥ silhouette score cao nh·∫•t)
optimal_k = K_range[np.argmax(silhouette_scores)]
print(f"S·ªë c·ª•m t·ªëi ∆∞u: {optimal_k} (Silhouette Score: {max(silhouette_scores):.4f})")

# Th·ª±c hi·ªán clustering v·ªõi K t·ªëi ∆∞u
kmeans_optimal = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
cluster_labels = kmeans_optimal.fit_predict(X_cluster_scaled)

# Th√™m cluster labels v√†o dataframe
df_encoded['Cluster'] = cluster_labels

# Ph√¢n t√≠ch clusters
axes[2].scatter(df_encoded['tenure'], df_encoded['MonthlyCharges'], 
               c=cluster_labels, cmap='viridis', alpha=0.6)
axes[2].set_xlabel('Tenure')
axes[2].set_ylabel('Monthly Charges')
axes[2].set_title(f'Ph√¢n c·ª•m kh√°ch h√†ng (K={optimal_k})')

plt.tight_layout()
plt.show()

# Ph√¢n t√≠ch ƒë·∫∑c ƒëi·ªÉm t·ª´ng cluster
print("\nüìä ƒê·∫∂C ƒêI·ªÇM C√ÅC CLUSTER:")
for i in range(optimal_k):
    cluster_data = df_encoded[df_encoded['Cluster'] == i]
    churn_rate = cluster_data['Churn'].mean()
    
    print(f"\nüî∏ Cluster {i} ({len(cluster_data)} kh√°ch h√†ng):")
    print(f"   T·ª∑ l·ªá Churn: {churn_rate:.2%}")
    print(f"   Tenure trung b√¨nh: {cluster_data['tenure'].mean():.1f} th√°ng")
    print(f"   MonthlyCharges trung b√¨nh: ${cluster_data['MonthlyCharges'].mean():.2f}")
    print(f"   TotalCharges trung b√¨nh: ${cluster_data['TotalCharges'].mean():.2f}")
```

### 5.3 Results Visualization
```python
# Chuy·ªÉn ƒë·ªïi k·∫øt qu·∫£ sang DataFrame ƒë·ªÉ d·ªÖ ph√¢n t√≠ch
results_df = pd.DataFrame(results).T

print("üìä B·∫¢NG SO S√ÅNH CHI TI·∫æT C√ÅC M√î H√åNH:")
print(results_df.round(4))

# T√¨m m√¥ h√¨nh t·ªët nh·∫•t theo t·ª´ng metric
print(f"\nüèÜ M√î H√åNH T·ªêT NH·∫§T THEO T·ª™NG METRIC:")
print(f"   Accuracy:  {results_df['accuracy'].idxmax()} ({results_df['accuracy'].max():.4f})")
print(f"   Precision: {results_df['precision'].idxmax()} ({results_df['precision'].max():.4f})")
print(f"   Recall:    {results_df['recall'].idxmax()} ({results_df['recall'].max():.4f})")
print(f"   F1-Score:  {results_df['f1_score'].idxmax()} ({results_df['f1_score'].max():.4f})")

# Bi·ªÉu ƒë·ªì so s√°nh hi·ªáu su·∫•t
fig, axes = plt.subplots(2, 3, figsize=(18, 12))

# So s√°nh Accuracy
results_df['accuracy'].plot(kind='bar', ax=axes[0,0], color='skyblue')
axes[0,0].set_title('So s√°nh Accuracy')
axes[0,0].set_ylabel('Accuracy')
axes[0,0].tick_params(axis='x', rotation=45)
axes[0,0].set_ylim([results_df['accuracy'].min() - 0.05, 1.0])

# So s√°nh Precision
results_df['precision'].plot(kind='bar', ax=axes[0,1], color='lightgreen')
axes[0,1].set_title('So s√°nh Precision')
axes[0,1].set_ylabel('Precision')
axes[0,1].tick_params(axis='x', rotation=45)
axes[0,1].set_ylim([results_df['precision'].min() - 0.05, 1.0])

# So s√°nh Recall
results_df['recall'].plot(kind='bar', ax=axes[0,2], color='lightcoral')
axes[0,2].set_title('So s√°nh Recall')
axes[0,2].set_ylabel('Recall')
axes[0,2].tick_params(axis='x', rotation=45)
axes[0,2].set_ylim([results_df['recall'].min() - 0.05, 1.0])

# So s√°nh F1-Score
results_df['f1_score'].plot(kind='bar', ax=axes[1,0], color='gold')
axes[1,0].set_title('So s√°nh F1-Score')
axes[1,0].set_ylabel('F1-Score')
axes[1,0].tick_params(axis='x', rotation=45)
axes[1,0].set_ylim([results_df['f1_score'].min() - 0.05, 1.0])

# So s√°nh AUC (ch·ªâ cho models c√≥ AUC)
auc_data = results_df[results_df['auc'].notna()]['auc']
if len(auc_data) > 0:
    auc_data.plot(kind='bar', ax=axes[1,1], color='plum')
    axes[1,1].set_title('So s√°nh AUC Score')
    axes[1,1].set_ylabel('AUC')
    axes[1,1].tick_params(axis='x', rotation=45)
    axes[1,1].set_ylim([auc_data.min() - 0.05, 1.0])

# Cross Validation Scores v·ªõi error bars
cv_data = results_df[results_df['cv_mean'].notna()]
if len(cv_data) > 0:
    cv_means = cv_data['cv_mean']
    cv_stds = cv_data['cv_std']
    x_pos = range(len(cv_means))
    axes[1,2].bar(x_pos, cv_means, yerr=cv_stds, capsize=5, color='orange', alpha=0.7)
    axes[1,2].set_title('Cross Validation Scores')
    axes[1,2].set_ylabel('CV Accuracy')
    axes[1,2].set_xticks(x_pos)
    axes[1,2].set_xticklabels(cv_means.index, rotation=45)

plt.tight_layout()
plt.show()

# Ma tr·∫≠n nh·∫ßm l·∫´n cho top 3 models theo accuracy
print(f"\nüéØ MA TR·∫¨N NH·∫¶M L·∫™N CHO TOP 3 M√î H√åNH:")
top_3_models = results_df.nlargest(3, 'accuracy').index

fig, axes = plt.subplots(1, min(3, len(top_3_models)), figsize=(15, 4))
if len(top_3_models) == 1:
    axes = [axes]

for i, model_name in enumerate(top_3_models[:3]):
    # L·∫•y predictions t·ª´ model
    model = multi_model.models[model_name]
    y_pred = model.predict(X_test_scaled)
    
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', ax=axes[i], cmap='Blues')
    axes[i].set_title(f'{model_name}\nAccuracy: {results[model_name]["accuracy"]:.3f}')
    axes[i].set_xlabel('D·ª± ƒëo√°n')
    axes[i].set_ylabel('Th·ª±c t·∫ø')

plt.tight_layout()
plt.show()

# Bi·ªÉu ƒë·ªì radar cho so s√°nh t·ªïng quan
import numpy as np

def create_radar_chart(models_data, metrics=['accuracy', 'precision', 'recall', 'f1_score']):
    fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(projection='polar'))
    
    # S·ªë l∆∞·ª£ng metrics
    num_metrics = len(metrics)
    angles = np.linspace(0, 2 * np.pi, num_metrics, endpoint=False).tolist()
    angles += angles[:1]  # ƒê√≥ng v√≤ng tr√≤n
    
    # V·∫Ω cho t·ª´ng model
    colors = ['red', 'blue', 'green', 'orange', 'purple']
    for i, (model_name, data) in enumerate(models_data.items()):
        values = [data[metric] for metric in metrics]
        values += values[:1]  # ƒê√≥ng v√≤ng tr√≤n
        
        ax.plot(angles, values, 'o-', linewidth=2, label=model_name, color=colors[i % len(colors)])
        ax.fill(angles, values, alpha=0.25, color=colors[i % len(colors)])
    
    # Thi·∫øt l·∫≠p labels
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels([metric.title() for metric in metrics])
    ax.set_ylim(0, 1)
    ax.set_title('So s√°nh t·ªïng quan c√°c m√¥ h√¨nh', size=16, y=1.1)
    ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))
    
    plt.tight_layout()
    plt.show()

# T·∫°o radar chart
print(f"\nüìà BI·ªÇU ƒê·ªí RADAR SO S√ÅNH T·ªîNG QUAN:")
create_radar_chart(results)
```

### 5.4 Market Basket Analysis (Lecture 12)
```python
# Lecture 12: Khai ph√° t·∫≠p m·ª•c th∆∞·ªùng xuy√™n v√† c√°c lu·∫≠t k·∫øt h·ª£p
print("\nüõí PH√ÇN T√çCH T·∫¨P M·ª§C TH∆Ø·ªúNG XUY√äN (MARKET BASKET ANALYSIS):")

# C√†i ƒë·∫∑t th∆∞ vi·ªán mlxtend n·∫øu ch∆∞a c√≥
try:
    from mlxtend.frequent_patterns import apriori, association_rules
    from mlxtend.preprocessing import TransactionEncoder
except ImportError:
    print("C√†i ƒë·∫∑t mlxtend...")
    import subprocess
    subprocess.run(["pip", "install", "mlxtend"], check=True)
    from mlxtend.frequent_patterns import apriori, association_rules
    from mlxtend.preprocessing import TransactionEncoder

# T·∫°o d·ªØ li·ªáu giao d·ªãch t·ª´ c√°c d·ªãch v·ª•
service_cols = ['PhoneService', 'MultipleLines', 'InternetService_DSL', 'InternetService_Fiber optic',
               'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 
               'StreamingTV', 'StreamingMovies']

# L·ªçc c√°c c·ªôt d·ªãch v·ª• c√≥ trong d·ªØ li·ªáu
available_service_cols = [col for col in service_cols if col in df_encoded.columns]
print(f"C√°c d·ªãch v·ª• ph√¢n t√≠ch: {available_service_cols}")

# T·∫°o transactions (ch·ªâ l·∫•y c√°c d·ªãch v·ª• ƒë∆∞·ª£c s·ª≠ d·ª•ng = 1)
transactions = []
for _, row in df_encoded.iterrows():
    transaction = []
    for col in available_service_cols:
        if col in df_encoded.columns and row[col] == 1:
            transaction.append(col)
    
    # Th√™m th√¥ng tin churn
    if row['Churn'] == 1:
        transaction.append('Churn_Yes')
    else:
        transaction.append('Churn_No')
    
    # Th√™m th√¥ng tin contract
    if 'Contract_One year' in df_encoded.columns and row['Contract_One year'] == 1:
        transaction.append('Contract_OneYear')
    elif 'Contract_Two year' in df_encoded.columns and row['Contract_Two year'] == 1:
        transaction.append('Contract_TwoYear')
    else:
        transaction.append('Contract_MonthToMonth')
    
    transactions.append(transaction)

# Chuy·ªÉn ƒë·ªïi th√†nh format cho apriori
te = TransactionEncoder()
te_ary = te.fit(transactions).transform(transactions)
df_transactions = pd.DataFrame(te_ary, columns=te.columns_)

print(f"S·ªë l∆∞·ª£ng transactions: {len(df_transactions)}")
print(f"S·ªë l∆∞·ª£ng items: {len(te.columns_)}")
print(f"Items: {list(te.columns_)}")

# T√¨m frequent itemsets
frequent_itemsets = apriori(df_transactions, min_support=0.1, use_colnames=True)
frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))

print(f"\nüìä S·ªê L∆Ø·ª¢NG FREQUENT ITEMSETS:")
itemset_counts = frequent_itemsets['length'].value_counts().sort_index()
print(itemset_counts)

# Hi·ªÉn th·ªã top frequent itemsets
print(f"\nüîù TOP 10 FREQUENT ITEMSETS:")
top_itemsets = frequent_itemsets.nlargest(10, 'support')
for idx, row in top_itemsets.iterrows():
    items = ', '.join(list(row['itemsets']))
    print(f"Support: {row['support']:.3f} | Items: {items}")

# T·∫°o association rules
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1.0)

if len(rules) > 0:
    # S·∫Øp x·∫øp theo lift
    rules_sorted = rules.sort_values('lift', ascending=False)
    
    print(f"\nüîó TOP 10 ASSOCIATION RULES (s·∫Øp x·∫øp theo Lift):")
    for idx, rule in rules_sorted.head(10).iterrows():
        antecedent = ', '.join(list(rule['antecedents']))
        consequent = ', '.join(list(rule['consequents']))
        print(f"Rule: {antecedent} ‚Üí {consequent}")
        print(f"   Support: {rule['support']:.3f}, Confidence: {rule['confidence']:.3f}, Lift: {rule['lift']:.3f}")
        print()
    
    # Ph√¢n t√≠ch rules li√™n quan ƒë·∫øn Churn
    churn_rules = rules_sorted[rules_sorted['consequents'].astype(str).str.contains('Churn')]
    
    if len(churn_rules) > 0:
        print(f"\n‚ö†Ô∏è  TOP 5 RULES D·∫™N ƒê·∫æN CHURN:")
        for idx, rule in churn_rules.head(5).iterrows():
            antecedent = ', '.join(list(rule['antecedents']))
            consequent = ', '.join(list(rule['consequents']))
            print(f"Rule: {antecedent} ‚Üí {consequent}")
            print(f"   Confidence: {rule['confidence']:.3f}, Lift: {rule['lift']:.3f}")
            print()
    
    # Visualization
    plt.figure(figsize=(15, 5))
    
    plt.subplot(1, 3, 1)
    plt.scatter(rules['support'], rules['confidence'], alpha=0.6, c=rules['lift'])
    plt.xlabel('Support')
    plt.ylabel('Confidence')
    plt.title('Support vs Confidence')
    plt.colorbar(label='Lift')
    
    plt.subplot(1, 3, 2)
    plt.hist(rules['lift'], bins=20, alpha=0.7, color='skyblue')
    plt.xlabel('Lift')
    plt.ylabel('Frequency')
    plt.title('Ph√¢n b·ªë Lift')
    
    plt.subplot(1, 3, 3)
    itemset_counts.plot(kind='bar', color='lightgreen')
    plt.xlabel('ƒê·ªô d√†i Itemset')
    plt.ylabel('S·ªë l∆∞·ª£ng')
    plt.title('Ph√¢n b·ªë ƒë·ªô d√†i Itemset')
    
    plt.tight_layout()
    plt.show()
else:
    print("Kh√¥ng t√¨m th·∫•y association rules v·ªõi threshold hi·ªán t·∫°i.")
```

## üéØ B∆∞·ªõc 6: Feature Selection Performance

### 6.1 Test v·ªõi Different Feature Sets
```python
# Ki·ªÉm tra hi·ªáu su·∫•t v·ªõi c√°c b·ªô ƒë·∫∑c tr∆∞ng kh√°c nhau
feature_sets = {
    'Top 5 Univariate': select_features_univariate(X, y, 5)[0],
    'Top 10 Univariate': select_features_univariate(X, y, 10)[0],
    'Top 15 Univariate': select_features_univariate(X, y, 15)[0],
    'Top 5 RF': select_features_rf(X, y, 5)[0],
    'Top 10 RF': select_features_rf(X, y, 10)[0],
    'Top 15 RF': select_features_rf(X, y, 15)[0],
}

# Hi·ªáu su·∫•t v·ªõi c√°c t·∫≠p con ƒë·∫∑c tr∆∞ng
subset_results = {}

for subset_name, features in feature_sets.items():
    print(f"\nüîç ƒêang ki·ªÉm tra {subset_name} ({len(features)} ƒë·∫∑c tr∆∞ng)...")
    
    # T·∫°o t·∫≠p con d·ªØ li·ªáu
    X_subset = X[features]
    X_train_sub, X_test_sub, y_train_sub, y_test_sub = train_test_split(
        X_subset, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # Chu·∫©n h√≥a
    X_train_sub_scaled = scaler.fit_transform(X_train_sub)
    X_test_sub_scaled = scaler.transform(X_test_sub)
    
    # Hu·∫•n luy·ªán m√¥ h√¨nh t·ªët nh·∫•t (Random Forest)
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train_sub_scaled, y_train_sub)
    
    # ƒê√°nh gi√°
    y_pred_sub = model.predict(X_test_sub_scaled)
    accuracy_sub = accuracy_score(y_test_sub, y_pred_sub)
    
    subset_results[subset_name] = {
        'n_features': len(features),
        'accuracy': accuracy_sub,
        'features': features
    }
    
    print(f"   ƒê·ªô ch√≠nh x√°c: {accuracy_sub:.4f} v·ªõi {len(features)} ƒë·∫∑c tr∆∞ng")

# Bi·ªÉu ƒë·ªì k·∫øt qu·∫£ l·ª±a ch·ªçn ƒë·∫∑c tr∆∞ng
subset_df = pd.DataFrame(subset_results).T
subset_df = subset_df.sort_values('accuracy', ascending=False)

plt.figure(figsize=(12, 6))
bars = plt.bar(range(len(subset_df)), subset_df['accuracy'], color='lightcoral')
plt.xlabel('Ph∆∞∆°ng ph√°p l·ª±a ch·ªçn ƒë·∫∑c tr∆∞ng')
plt.ylabel('ƒê·ªô ch√≠nh x√°c')
plt.title('Hi·ªáu su·∫•t theo ph∆∞∆°ng ph√°p l·ª±a ch·ªçn ƒë·∫∑c tr∆∞ng')
plt.xticks(range(len(subset_df)), subset_df.index, rotation=45)

# Th√™m nh√£n gi√° tr·ªã tr√™n c·ªôt
for i, bar in enumerate(bars):
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height + 0.005,
             f'{height:.3f}\n({subset_df.iloc[i]["n_features"]} ƒë·∫∑c tr∆∞ng)',
             ha='center', va='bottom')

plt.tight_layout()
plt.show()

print("\nüèÜ K·∫æT QU·∫¢ CH·ªåN ƒê·∫∂C TR∆ØNG T·ªêT NH·∫§T:")
print(subset_df.sort_values('accuracy', ascending=False))
```

## üíæ B∆∞·ªõc 7: Save Results

### 7.1 Export Results
```python
# L∆∞u k·∫øt qu·∫£ ra CSV
results_df.to_csv('model_comparison_results.csv')
subset_df.to_csv('feature_selection_results.csv')

# T·∫£i xu·ªëng c√°c file
from google.colab import files
files.download('model_comparison_results.csv')
files.download('feature_selection_results.csv')

print("‚úÖ K·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u v√† t·∫£i xu·ªëng!")
```

## üéâ Ho√†n th√†nh!

### ÔøΩÔøΩ Summary
```python
# T√≥m t·∫Øt c·∫£i ti·∫øn - ch·ªçn m√¥ h√¨nh theo business metrics
def select_best_model_for_churn(results_dict):
    """
    Ch·ªçn m√¥ h√¨nh t·ªët nh·∫•t cho b√†i to√°n churn prediction
    ∆Øu ti√™n: Recall > AUC > F1-Score > Accuracy
    """
    scores = {}
    for model, metrics in results_dict.items():
        # Weighted business score for churn prediction
        business_score = (
            metrics['recall'] * 0.35 +      # Quan tr·ªçng nh·∫•t: ph√°t hi·ªán churn
            metrics['auc'] * 0.25 +         # Kh·∫£ nƒÉng ph√¢n bi·ªát
            metrics['f1_score'] * 0.25 +    # C√¢n b·∫±ng precision/recall  
            metrics['accuracy'] * 0.15      # ƒê·ªô ch√≠nh x√°c t·ªïng th·ªÉ
        )
        scores[model] = business_score
    
    best_model = max(scores, key=scores.get)
    return best_model, scores

# S·ª≠ d·ª•ng function
best_model, business_scores = select_best_model_for_churn(results)

print("üéØ T√ìM T·∫ÆT D·ª∞ √ÅN")
print("=" * 50)
print(f"üìä D·ªØ li·ªáu: {df.shape[0]} kh√°ch h√†ng, {df.shape[1]} ƒë·∫∑c tr∆∞ng")
print(f"üéØ M·ª•c ti√™u: Customer Churn ({(y.sum() / len(y) * 100):.1f}% t·ª∑ l·ªá churn)")
print(f"üîß ƒê·∫∑c tr∆∞ng sau ti·ªÅn x·ª≠ l√Ω: {X.shape[1]}")
print(f"üèÜ M√¥ h√¨nh t·ªët nh·∫•t (Accuracy): {results_df['accuracy'].idxmax()} ({results_df['accuracy'].max():.3f})")
print(f"üéØ M√¥ h√¨nh t·ªët nh·∫•t (Business): {best_model} (Score: {business_scores[best_model]:.3f})")
print(f"üìä B·ªô ƒë·∫∑c tr∆∞ng t·ªët nh·∫•t: {subset_df.index[0]} ({subset_df.iloc[0]['accuracy']:.3f})")

print(f"\nüìà BUSINESS SCORES CHO T·∫§T C·∫¢ M√î H√åNH:")
for model, score in sorted(business_scores.items(), key=lambda x: x[1], reverse=True):
    print(f"   {model}: {score:.3f}")

print("\n‚úÖ Ph√¢n t√≠ch ho√†n th√†nh th√†nh c√¥ng!")
```