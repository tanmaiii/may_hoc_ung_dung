# ğŸš€ HÆ¯á»šNG DáºªN CHáº Y PROJECT TRÃŠN GOOGLE COLAB

## ğŸ“‹ Tá»•ng quan
HÆ°á»›ng dáº«n chi tiáº¿t Ä‘á»ƒ cháº¡y project **Telco Customer Churn Prediction** trÃªn Google Colab má»™t cÃ¡ch Ä‘Æ¡n giáº£n vÃ  hiá»‡u quáº£.

## âš¡ Æ¯u Ä‘iá»ƒm cá»§a Google Colab
- âœ… **Miá»…n phÃ­** vÃ  cÃ³ GPU/TPU
- âœ… **KhÃ´ng cáº§n cÃ i Ä‘áº·t** gÃ¬ trÃªn mÃ¡y tÃ­nh
- âœ… **ÄÃ£ cÃ³ sáºµn** táº¥t cáº£ thÆ° viá»‡n cáº§n thiáº¿t
- âœ… **Dá»… chia sáº»** vÃ  collaborate
- âœ… **LÆ°u trá»¯ trÃªn Google Drive**

## ğŸ”§ BÆ°á»›c 1: Chuáº©n bá»‹

### 1.1 Táº¡o Google Colab Notebook
1. Truy cáº­p [Google Colab](https://colab.research.google.com/)
2. ÄÄƒng nháº­p tÃ i khoáº£n Google
3. Táº¡o notebook má»›i: **File > New notebook**

### 1.2 Upload dá»¯ liá»‡u
```python
import pandas as pd
# Náº¿u cÃ³ link download trá»±c tiáº¿p
url = "https://raw.githubusercontent.com/tanmaiii/may_hoc_ung_dung/refs/heads/main/telco-customer-churn.csv"
df = pd.read_csv(url)
```

## ğŸ“Š BÆ°á»›c 2: Notebook hoÃ n chá»‰nh

### 2.1 Setup vÃ  Import Libraries
```python
# CÃ i Ä‘áº·t thÃªm packages náº¿u cáº§n
!pip install plotly seaborn

# Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Machine Learning
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve

# Táº¯t warnings
import warnings
warnings.filterwarnings('ignore')

# CÃ i Ä‘áº·t style cho plots
plt.style.use('default')
sns.set_palette("husl")
```

### 2.2 Load vÃ  Explore Data (Load dá»¯ liá»‡u)
```python
# Load data
df = pd.read_csv('telco-customer-churn.csv')  # Hoáº·c Ä‘Æ°á»ng dáº«n file báº¡n upload

print("ğŸ¯ Tá»”NG QUAN Dá»® LIá»†U")
print(f"KÃ­ch thÆ°á»›c: {df.shape}")
print(f"GiÃ¡ trá»‹ thiáº¿u: {df.isnull().sum().sum()}")
print("\nğŸ“Š 5 dÃ²ng Ä‘áº§u tiÃªn:")
df.head()
```

### 2.3 Data Analysis vÃ  Visualization (PhÃ¢n tÃ­ch dá»¯ liá»‡u vÃ  trá»±c quan hÃ³a)
```python
# Basic info
print("ğŸ“ˆ THÃ”NG TIN Dá»® LIá»†U")
df.info()

print("\nğŸ“Š THá»NG KÃŠ MÃ” Táº¢")
df.describe()

# Churn distribution
print("\nğŸ¯ PHÃ‚N Bá» CHURN")
churn_counts = df['Churn'].value_counts()
print(churn_counts)
print(f"Tá»· lá»‡ khÃ¡ch hÃ ng rá»i bá» dá»‹ch vá»¥ (Churn rate): {churn_counts[1] / len(df) * 100:.2f}%")

# Visualization
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# Biá»ƒu Ä‘á»“ pie char
axes[0,0].pie(churn_counts.values, labels=churn_counts.index, autopct='%1.1f%%')
axes[0,0].set_title('Tá»· lá»‡ khÃ¡ch hÃ ng rá»i bá» (Churn)')

# Biá»ƒu Ä‘á»“ Histogram
axes[0,1].hist(df['tenure'], bins=30, alpha=0.7)
axes[0,1].set_title('PhÃ¢n bá»‘ thá»i gian sá»­ dá»¥ng')
axes[0,1].set_xlabel('Sá»‘ thÃ¡ng sá»­ dá»¥ng (tenure)')

# Biá»ƒu Ä‘á»“ Boxplot
sns.boxplot(data=df, x='Churn', y='MonthlyCharges', ax=axes[1,0])
axes[1,0].set_title('Chi phÃ­ hÃ ng thÃ¡ng theo Churn')

# Biá»ƒu Ä‘á»“ Bar chart
contract_churn = pd.crosstab(df['Contract'], df['Churn'])
contract_churn.plot(kind='bar', ax=axes[1,1])
axes[1,1].set_title('Loáº¡i há»£p Ä‘á»“ng vÃ  Churn')
axes[1,1].legend(title='Churn')

plt.tight_layout()
plt.show()
```

## ğŸ”„ BÆ°á»›c 3: Data Preprocessing (Tiá»n xá»­ lÃ½ dá»¯ liá»‡u)

### 3.1 Data Cleaning (LÃ m sáº¡ch)
```python
# Táº¡o copy Ä‘á»ƒ xá»­ lÃ½
df_clean = df.copy()

print("ğŸ§¹ LÃ€M Sáº CH Dá»® LIá»†U...")

# Xá»­ lÃ½ TotalCharges (cÃ³ giÃ¡ trá»‹ ' ')
print(f"GiÃ¡ trá»‹ ' ' trong TotalCharges: {(df_clean['TotalCharges'] == ' ').sum()}")
df_clean['TotalCharges'] = df_clean['TotalCharges'].replace(' ', np.nan)
df_clean['TotalCharges'] = pd.to_numeric(df_clean['TotalCharges'], errors='coerce')

# Fill missing values
df_clean['TotalCharges'].fillna(0, inplace=True)

# Remove customerID
df_clean = df_clean.drop('customerID', axis=1)

print(f"âœ… KÃ­ch thÆ°á»›c sau lÃ m sáº¡ch: {df_clean.shape}")
print(f"GiÃ¡ trá»‹ thiáº¿u sau lÃ m sáº¡ch: {df_clean.isnull().sum().sum()}")
```

### 3.2 Feature Engineering 
```python
print("ğŸ”§ Táº O Äáº¶C TRÆ¯NG Má»šI...")

# Binary encoding cho Yes/No columns
binary_cols = []
for col in df_clean.columns:
    if df_clean[col].dtype == 'object' and col != 'Churn':
        unique_vals = df_clean[col].unique()
        if set(unique_vals).issubset({'Yes', 'No'}):
            binary_cols.append(col)
            df_clean[col] = df_clean[col].map({'Yes': 1, 'No': 0})

print(f"CÃ¡c cá»™t Ä‘Ã£ mÃ£ hÃ³a nhá»‹ phÃ¢n: {binary_cols}")

# One-hot encoding cho nominal variables
nominal_cols = ['gender', 'Contract', 'PaymentMethod', 'InternetService']
df_encoded = pd.get_dummies(df_clean, columns=nominal_cols, drop_first=True)

# Encode target variable
df_encoded['Churn'] = df_encoded['Churn'].map({'Yes': 1, 'No': 0})

# Create new features
df_encoded['AvgChargePerTenure'] = np.where(
    df_encoded['tenure'] > 0,
    df_encoded['TotalCharges'] / df_encoded['tenure'],
    df_encoded['MonthlyCharges']
)

# Tenure groups - táº¡o nhÃ³m theo thá»i gian sá»­ dá»¥ng
try:
    df_encoded['TenureGroup'] = pd.cut(
        df_encoded['tenure'],
        bins=[0, 12, 36, 60, float('inf')],
        labels=['0-12m', '13-36m', '37-60m', '60m+']
    )
    # Encode categorical labels thÃ nh sá»‘
    df_encoded['TenureGroup'] = df_encoded['TenureGroup'].cat.codes
except Exception as e:
    print(f"Lá»—i khi táº¡o TenureGroup: {e}")
    # Fallback: táº¡o nhÃ³m Ä‘Æ¡n giáº£n
    df_encoded['TenureGroup'] = pd.cut(
        df_encoded['tenure'],
        bins=4,
        labels=[0, 1, 2, 3]
    ).astype(int)

print(f"âœ… KÃ­ch thÆ°á»›c cuá»‘i cÃ¹ng: {df_encoded.shape}")
df_encoded.head()
```

## ğŸ¯ BÆ°á»›c 4: Feature Selection

### 4.1 Correlation Analysis
```python
# Correlation heatmap
plt.figure(figsize=(20, 16))
correlation_matrix = df_encoded.corr()
sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', center=0)
plt.title('Ma tráº­n tÆ°Æ¡ng quan Ä‘áº·c trÆ°ng')
plt.show()

# Top correlations with target
target_corr = correlation_matrix['Churn'].abs().sort_values(ascending=False)
print("ğŸ¯ TOP 15 Äáº¶C TRÆ¯NG TÆ¯Æ NG QUAN Vá»šI CHURN:")
print(target_corr.head(15))
```

### 4.2 Feature Selection Methods
```python
# Prepare data
X = df_encoded.drop('Churn', axis=1)
y = df_encoded['Churn']

print(f"KÃ­ch thÆ°á»›c Ä‘áº·c trÆ°ng: {X.shape}")
print(f"PhÃ¢n bá»‘ target: {y.value_counts().to_dict()}")

# Method 1: SelectKBest
def select_features_univariate(X, y, k=10):
    selector = SelectKBest(score_func=f_classif, k=k)
    X_selected = selector.fit_transform(X, y)
    selected_features = X.columns[selector.get_support()].tolist()
    scores = selector.scores_
    
    feature_scores = pd.DataFrame({
        'feature': X.columns,
        'score': scores
    }).sort_values('score', ascending=False)
    
    return selected_features, feature_scores

# Method 2: Random Forest Feature Importance
def select_features_rf(X, y, k=10):
    rf = RandomForestClassifier(n_estimators=100, random_state=42)
    rf.fit(X, y)
    
    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': rf.feature_importances_
    }).sort_values('importance', ascending=False)
    
    selected_features = feature_importance.head(k)['feature'].tolist()
    return selected_features, feature_importance

# Apply feature selection
for k in [5, 10, 15]:
    print(f"\nğŸ” TOP {k} Äáº¶C TRÆ¯NG:")
    
    # Univariate selection
    features_uni, scores_uni = select_features_univariate(X, y, k)
    print(f"PhÆ°Æ¡ng phÃ¡p Univariate: {features_uni}")
    
    # Random Forest
    features_rf, scores_rf = select_features_rf(X, y, k)
    print(f"Random Forest: {features_rf}")
```

## ğŸ¤– BÆ°á»›c 5: Model Training vÃ  Evaluation

### 5.1 Data Splitting vÃ  Scaling
```python
# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"Táº­p huáº¥n luyá»‡n: {X_train_scaled.shape}")
print(f"Táº­p kiá»ƒm tra: {X_test_scaled.shape}")
print(f"PhÃ¢n bá»‘ target trong táº­p train: {y_train.value_counts(normalize=True).round(3).to_dict()}")
```

### 5.2 Model Training
```python
# Define models
models = {
    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'SVM': SVC(random_state=42, probability=True)
}

# Train and evaluate models
results = {}

for name, model in models.items():
    print(f"\nğŸ”„ Äang huáº¥n luyá»‡n {name}...")
    
    # Train model
    model.fit(X_train_scaled, y_train)
    
    # Predictions
    y_pred = model.predict(X_test_scaled)
    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, 'predict_proba') else None
    
    # Metrics
    accuracy = accuracy_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else None
    
    # Cross validation
    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')
    
    results[name] = {
        'accuracy': accuracy,
        'auc': auc,
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std(),
        'y_pred': y_pred,
        'y_pred_proba': y_pred_proba
    }
    
    print(f"   Äá»™ chÃ­nh xÃ¡c: {accuracy:.4f}")
    print(f"   AUC: {auc:.4f}" if auc else "   AUC: KhÃ´ng cÃ³")
    print(f"   CV Accuracy: {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})")
```

### 5.3 Results Visualization
```python
# Results comparison
results_df = pd.DataFrame({
    name: {
        'Test Accuracy': results[name]['accuracy'],
        'AUC Score': results[name]['auc'] if results[name]['auc'] else 0,
        'CV Mean': results[name]['cv_mean'],
        'CV Std': results[name]['cv_std']
    } for name in results.keys()
}).T

print("ğŸ“Š SO SÃNH MÃ” HÃŒNH:")
print(results_df.round(4))

# Plot comparison
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# Accuracy comparison
results_df['Test Accuracy'].plot(kind='bar', ax=axes[0], color='skyblue')
axes[0].set_title('So sÃ¡nh Ä‘á»™ chÃ­nh xÃ¡c kiá»ƒm tra')
axes[0].set_ylabel('Äá»™ chÃ­nh xÃ¡c')
axes[0].tick_params(axis='x', rotation=45)

# AUC comparison
results_df['AUC Score'].plot(kind='bar', ax=axes[1], color='lightgreen')
axes[1].set_title('So sÃ¡nh Ä‘iá»ƒm AUC')
axes[1].set_ylabel('AUC')
axes[1].tick_params(axis='x', rotation=45)

# CV scores with error bars
cv_means = results_df['CV Mean']
cv_stds = results_df['CV Std']
axes[2].bar(range(len(cv_means)), cv_means, yerr=cv_stds, capsize=5, color='orange', alpha=0.7)
axes[2].set_title('Äiá»ƒm Cross Validation')
axes[2].set_ylabel('CV Accuracy')
axes[2].set_xticks(range(len(cv_means)))
axes[2].set_xticklabels(cv_means.index, rotation=45)

plt.tight_layout()
plt.show()

# Confusion matrices
fig, axes = plt.subplots(1, 3, figsize=(15, 4))
for i, (name, result) in enumerate(results.items()):
    cm = confusion_matrix(y_test, result['y_pred'])
    sns.heatmap(cm, annot=True, fmt='d', ax=axes[i], cmap='Blues')
    axes[i].set_title(f'{name}\nÄá»™ chÃ­nh xÃ¡c: {result["accuracy"]:.3f}')
    axes[i].set_xlabel('Dá»± Ä‘oÃ¡n')
    axes[i].set_ylabel('Thá»±c táº¿')

plt.tight_layout()
plt.show()
```

## ğŸ¯ BÆ°á»›c 6: Feature Selection Performance

### 6.1 Test vá»›i Different Feature Sets
```python
# Test performance vá»›i different feature sets
feature_sets = {
    'Top 5 Univariate': select_features_univariate(X, y, 5)[0],
    'Top 10 Univariate': select_features_univariate(X, y, 10)[0],
    'Top 15 Univariate': select_features_univariate(X, y, 15)[0],
    'Top 5 RF': select_features_rf(X, y, 5)[0],
    'Top 10 RF': select_features_rf(X, y, 10)[0],
    'Top 15 RF': select_features_rf(X, y, 15)[0],
}

# Performance vá»›i feature subsets
subset_results = {}

for subset_name, features in feature_sets.items():
    print(f"\nğŸ” Äang kiá»ƒm tra {subset_name} ({len(features)} Ä‘áº·c trÆ°ng)...")
    
    # Subset data
    X_subset = X[features]
    X_train_sub, X_test_sub, y_train_sub, y_test_sub = train_test_split(
        X_subset, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # Scale
    X_train_sub_scaled = scaler.fit_transform(X_train_sub)
    X_test_sub_scaled = scaler.transform(X_test_sub)
    
    # Train best model (Random Forest)
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train_sub_scaled, y_train_sub)
    
    # Evaluate
    y_pred_sub = model.predict(X_test_sub_scaled)
    accuracy_sub = accuracy_score(y_test_sub, y_pred_sub)
    
    subset_results[subset_name] = {
        'n_features': len(features),
        'accuracy': accuracy_sub,
        'features': features
    }
    
    print(f"   Äá»™ chÃ­nh xÃ¡c: {accuracy_sub:.4f} vá»›i {len(features)} Ä‘áº·c trÆ°ng")

# Plot feature selection results
subset_df = pd.DataFrame(subset_results).T
subset_df = subset_df.sort_values('accuracy', ascending=False)

plt.figure(figsize=(12, 6))
bars = plt.bar(range(len(subset_df)), subset_df['accuracy'], color='lightcoral')
plt.xlabel('PhÆ°Æ¡ng phÃ¡p lá»±a chá»n Ä‘áº·c trÆ°ng')
plt.ylabel('Äá»™ chÃ­nh xÃ¡c')
plt.title('Hiá»‡u suáº¥t theo phÆ°Æ¡ng phÃ¡p lá»±a chá»n Ä‘áº·c trÆ°ng')
plt.xticks(range(len(subset_df)), subset_df.index, rotation=45)

# Add value labels on bars
for i, bar in enumerate(bars):
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height + 0.005,
             f'{height:.3f}\n({subset_df.iloc[i]["n_features"]} Ä‘áº·c trÆ°ng)',
             ha='center', va='bottom')

plt.tight_layout()
plt.show()

print("\nğŸ† Káº¾T QUáº¢ CHá»ŒN Äáº¶C TRÆ¯NG Tá»T NHáº¤T:")
print(subset_df.sort_values('accuracy', ascending=False))
```

## ğŸ’¾ BÆ°á»›c 7: Save Results

### 7.1 Export Results
```python
# Save results to CSV
results_df.to_csv('model_comparison_results.csv')
subset_df.to_csv('feature_selection_results.csv')

# Download files
from google.colab import files
files.download('model_comparison_results.csv')
files.download('feature_selection_results.csv')

print("âœ… Káº¿t quáº£ Ä‘Ã£ Ä‘Æ°á»£c lÆ°u vÃ  táº£i xuá»‘ng!")
```

## ğŸ‰ HoÃ n thÃ nh!

### ğŸ“Š Summary
```python
print("ğŸ¯ TÃ“M Táº®T Dá»° ÃN")
print("=" * 50)
print(f"ğŸ“Š Dá»¯ liá»‡u: {df.shape[0]} khÃ¡ch hÃ ng, {df.shape[1]} Ä‘áº·c trÆ°ng")
print(f"ğŸ¯ Má»¥c tiÃªu: Customer Churn ({(y.sum() / len(y) * 100):.1f}% tá»· lá»‡ churn)")
print(f"ğŸ”§ Äáº·c trÆ°ng sau tiá»n xá»­ lÃ½: {X.shape[1]}")
print(f"ğŸ† MÃ´ hÃ¬nh tá»‘t nháº¥t: {results_df['Test Accuracy'].idxmax()} ({results_df['Test Accuracy'].max():.3f})")
print(f"ğŸ¯ Bá»™ Ä‘áº·c trÆ°ng tá»‘t nháº¥t: {subset_df.index[0]} ({subset_df.iloc[0]['accuracy']:.3f})")
print("\nâœ… PhÃ¢n tÃ­ch hoÃ n thÃ nh thÃ nh cÃ´ng!")
```

## ğŸš€ Tips cho Google Colab

### Tá»‘i Æ°u Performance:
- Sá»­ dá»¥ng GPU: **Runtime > Change runtime type > GPU**
- Disconnect sau khi xong: **Runtime > Disconnect**
- Restart náº¿u memory Ä‘áº§y: **Runtime > Restart runtime**

### LÆ°u trá»¯:
- Mount Google Drive Ä‘á»ƒ lÆ°u permanent
- Download results quan trá»ng vá» mÃ¡y
- Copy notebook vÃ o Drive Ä‘á»ƒ backup

### Troubleshooting:
- Náº¿u bá»‹ disconnect: Re-run tá»« Ä‘áº§u
- Náº¿u thiáº¿u package: `!pip install package_name`
- Náº¿u lá»—i memory: Giáº£m data size hoáº·c dÃ¹ng sampling

---

**ğŸ‰ ChÃºc báº¡n thÃ nh cÃ´ng vá»›i project Machine Learning!** 