# ğŸš€ HÆ¯á»šNG DáºªN CHáº Y PROJECT TRÃŠN GOOGLE COLAB

## ğŸ“‹ Tá»•ng quan
HÆ°á»›ng dáº«n chi tiáº¿t Ä‘á»ƒ cháº¡y project **Telco Customer Churn Prediction** trÃªn Google Colab má»™t cÃ¡ch Ä‘Æ¡n giáº£n vÃ  hiá»‡u quáº£.

## âš¡ Æ¯u Ä‘iá»ƒm cá»§a Google Colab
- âœ… **Miá»…n phÃ­** vÃ  cÃ³ GPU/TPU
- âœ… **KhÃ´ng cáº§n cÃ i Ä‘áº·t** gÃ¬ trÃªn mÃ¡y tÃ­nh
- âœ… **ÄÃ£ cÃ³ sáºµn** táº¥t cáº£ thÆ° viá»‡n cáº§n thiáº¿t
- âœ… **Dá»… chia sáº»** vÃ  collaborate
- âœ… **LÆ°u trá»¯ trÃªn Google Drive**

## ğŸ”§ BÆ°á»›c 1: Chuáº©n bá»‹

### 1.1 Táº¡o Google Colab Notebook
1. Truy cáº­p [Google Colab](https://colab.research.google.com/)
2. ÄÄƒng nháº­p tÃ i khoáº£n Google
3. Táº¡o notebook má»›i: **File > New notebook**

### 1.2 Upload dá»¯ liá»‡u
```python
import pandas as pd
# Náº¿u cÃ³ liÃªn káº¿t táº£i xuá»‘ng trá»±c tiáº¿p
url = "https://raw.githubusercontent.com/tanmaiii/may_hoc_ung_dung/refs/heads/main/telco-customer-churn.csv"
df = pd.read_csv(url)
```

## ğŸ“Š BÆ°á»›c 2: Notebook hoÃ n chá»‰nh

### 2.1 Setup vÃ  Import Libraries
```python
# CÃ i Ä‘áº·t thÃªm gÃ³i thÆ° viá»‡n náº¿u cáº§n
!pip install plotly seaborn

# Import cÃ¡c thÆ° viá»‡n
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# MÃ¡y há»c
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve

# Táº¯t cáº£nh bÃ¡o
import warnings
warnings.filterwarnings('ignore')

# CÃ i Ä‘áº·t style cho biá»ƒu Ä‘á»“
plt.style.use('default')
sns.set_palette("husl")
```

### 2.2 Load vÃ  Explore Data (Load dá»¯ liá»‡u)
```python
# Táº£i dá»¯ liá»‡u
df = pd.read_csv('telco-customer-churn.csv')  # Hoáº·c Ä‘Æ°á»ng dáº«n file báº¡n upload

print("ğŸ¯ Tá»”NG QUAN Dá»® LIá»†U")
print(f"KÃ­ch thÆ°á»›c: {df.shape}")
print(f"GiÃ¡ trá»‹ thiáº¿u: {df.isnull().sum().sum()}")
print("\nğŸ“Š 5 dÃ²ng Ä‘áº§u tiÃªn:")
df.head()
```

### 2.3 Data Analysis vÃ  Visualization (PhÃ¢n tÃ­ch dá»¯ liá»‡u vÃ  trá»±c quan hÃ³a)
```python
# ThÃ´ng tin cÆ¡ báº£n
print("ğŸ“ˆ THÃ”NG TIN Dá»® LIá»†U")
df.info()

print("\nğŸ“Š THá»NG KÃŠ MÃ” Táº¢")
df.describe()

# PhÃ¢n bá»‘ Churn
print("\nğŸ¯ PHÃ‚N Bá» CHURN")
churn_counts = df['Churn'].value_counts()
print(churn_counts)
print(f"Tá»· lá»‡ khÃ¡ch hÃ ng rá»i bá» dá»‹ch vá»¥ (Churn rate): {churn_counts[1] / len(df) * 100:.2f}%")

# Trá»±c quan hÃ³a
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# Biá»ƒu Ä‘á»“ trÃ²n
axes[0,0].pie(churn_counts.values, labels=churn_counts.index, autopct='%1.1f%%')
axes[0,0].set_title('Tá»· lá»‡ khÃ¡ch hÃ ng rá»i bá» (Churn)')

# Biá»ƒu Ä‘á»“ phÃ¢n phá»‘i
axes[0,1].hist(df['tenure'], bins=30, alpha=0.7)
axes[0,1].set_title('PhÃ¢n bá»‘ thá»i gian sá»­ dá»¥ng')
axes[0,1].set_xlabel('Sá»‘ thÃ¡ng sá»­ dá»¥ng (tenure)')

# Biá»ƒu Ä‘á»“ há»™p
sns.boxplot(data=df, x='Churn', y='MonthlyCharges', ax=axes[1,0])
axes[1,0].set_title('Chi phÃ­ hÃ ng thÃ¡ng theo Churn')

# Biá»ƒu Ä‘á»“ cá»™t
contract_churn = pd.crosstab(df['Contract'], df['Churn'])
contract_churn.plot(kind='bar', ax=axes[1,1])
axes[1,1].set_title('Loáº¡i há»£p Ä‘á»“ng vÃ  Churn')
axes[1,1].legend(title='Churn')

plt.tight_layout()
plt.show()
```

## ğŸ”„ BÆ°á»›c 3: Data Preprocessing (Tiá»n xá»­ lÃ½ dá»¯ liá»‡u)

### 3.1 Data Cleaning (LÃ m sáº¡ch)
```python
# Táº¡o báº£n sao Ä‘á»ƒ xá»­ lÃ½
df_clean = df.copy()

print("ğŸ§¹ LÃ€M Sáº CH Dá»® LIá»†U...")

# Xá»­ lÃ½ TotalCharges (cÃ³ giÃ¡ trá»‹ ' ')
print(f"GiÃ¡ trá»‹ ' ' trong TotalCharges: {(df_clean['TotalCharges'] == ' ').sum()}")
df_clean['TotalCharges'] = df_clean['TotalCharges'].replace(' ', np.nan)
df_clean['TotalCharges'] = pd.to_numeric(df_clean['TotalCharges'], errors='coerce')

# Äiá»n giÃ¡ trá»‹ thiáº¿u
df_clean['TotalCharges'].fillna(0, inplace=True)

# Loáº¡i bá» customerID
df_clean = df_clean.drop('customerID', axis=1)

print(f"âœ… KÃ­ch thÆ°á»›c sau lÃ m sáº¡ch: {df_clean.shape}")
print(f"GiÃ¡ trá»‹ thiáº¿u sau lÃ m sáº¡ch: {df_clean.isnull().sum().sum()}")
```

### 3.2 Feature Engineering 
```python
print("ğŸ”§ Táº O Äáº¶C TRÆ¯NG Má»šI...")

# Kiá»ƒm tra cÃ¡c giÃ¡ trá»‹ unique trong táº¥t cáº£ cÃ¡c cá»™t object
print("ğŸ“Š KIá»‚M TRA CÃC GIÃ TRá»Š UNIQUE TRONG CÃC Cá»˜T:")
for col in df_clean.columns:
    if df_clean[col].dtype == 'object':
        unique_vals = df_clean[col].unique()
        print(f"{col}: {unique_vals}")

# Xá»­ lÃ½ cÃ¡c cá»™t cÃ³ giÃ¡ trá»‹ Ä‘áº·c biá»‡t (nhÆ° "No phone service", "No internet service")
special_service_cols = ['MultipleLines', 'OnlineSecurity', 'OnlineBackup', 
                       'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies']

for col in special_service_cols:
    if col in df_clean.columns:
        print(f"\nğŸ”§ Xá»­ lÃ½ cá»™t {col}...")
        # Chuyá»ƒn "No phone service" vÃ  "No internet service" thÃ nh "No"
        df_clean[col] = df_clean[col].replace({'No phone service': 'No', 
                                             'No internet service': 'No'})
        print(f"GiÃ¡ trá»‹ sau xá»­ lÃ½: {df_clean[col].unique()}")

# MÃ£ hÃ³a nhá»‹ phÃ¢n cho cÃ¡c cá»™t Yes/No
binary_cols = []
for col in df_clean.columns:
    if df_clean[col].dtype == 'object' and col != 'Churn':
        unique_vals = df_clean[col].unique()
        if set(unique_vals).issubset({'Yes', 'No'}):
            binary_cols.append(col)
            df_clean[col] = df_clean[col].map({'Yes': 1, 'No': 0})

print(f"\nâœ… CÃ¡c cá»™t Ä‘Ã£ mÃ£ hÃ³a nhá»‹ phÃ¢n: {binary_cols}")

# MÃ£ hÃ³a one-hot cho cÃ¡c biáº¿n Ä‘á»‹nh danh
nominal_cols = ['gender', 'Contract', 'PaymentMethod', 'InternetService']
df_encoded = pd.get_dummies(df_clean, columns=nominal_cols, drop_first=True)

# Tráº£ vá» 1, 0 thay vÃ¬ yes, no
df_encoded['Churn'] = df_encoded['Churn'].map({'Yes': 1, 'No': 0})

# Táº¡o cÃ¡c Ä‘áº·c trÆ°ng má»›i
df_encoded['AvgChargePerTenure'] = np.where(
    df_encoded['tenure'] > 0,
    df_encoded['TotalCharges'] / df_encoded['tenure'],
    df_encoded['MonthlyCharges']
)

# NhÃ³m thá»i gian sá»­ dá»¥ng - táº¡o nhÃ³m theo thá»i gian sá»­ dá»¥ng 
# 0: 0-12m (KhÃ¡ch hÃ ng má»›i)
# 1: 13-36m (KhÃ¡ch hÃ ng trung háº¡n)
# 2: 37-60m (KhÃ¡ch hÃ ng dÃ i háº¡n)
# 3: 60m+ (KhÃ¡ch hÃ ng trung thÃ nh)

try:
    df_encoded['TenureGroup'] = pd.cut(
        df_encoded['tenure'],
        bins=[0, 12, 36, 60, float('inf')],
        labels=['0-12m', '13-36m', '37-60m', '60m+']
    )
    # MÃ£ hÃ³a nhÃ£n phÃ¢n loáº¡i thÃ nh sá»‘
    df_encoded['TenureGroup'] = df_encoded['TenureGroup'].cat.codes
except Exception as e:
    print(f"Lá»—i khi táº¡o TenureGroup: {e}")
    # PhÆ°Æ¡ng Ã¡n dá»± phÃ²ng: táº¡o nhÃ³m Ä‘Æ¡n giáº£n
    df_encoded['TenureGroup'] = pd.cut(
        df_encoded['tenure'],
        bins=4,
        labels=[0, 1, 2, 3]
    ).astype(int)

# Kiá»ƒm tra cÃ¡c cá»™t cÃ²n láº¡i chÆ°a Ä‘Æ°á»£c mÃ£ hÃ³a
print(f"\nğŸ“‹ KIá»‚M TRA CÃC Cá»˜T SAU KHI MÃƒ HÃ“A:")
remaining_object_cols = [col for col in df_encoded.columns if df_encoded[col].dtype == 'object']
if remaining_object_cols:
    print(f"âš ï¸  CÃ¡c cá»™t chÆ°a Ä‘Æ°á»£c mÃ£ hÃ³a: {remaining_object_cols}")
    for col in remaining_object_cols:
        print(f"   {col}: {df_encoded[col].unique()}")
else:
    print("âœ… Táº¥t cáº£ cÃ¡c cá»™t Ä‘Ã£ Ä‘Æ°á»£c mÃ£ hÃ³a thÃ nh sá»‘!")

print(f"\nâœ… KÃ­ch thÆ°á»›c cuá»‘i cÃ¹ng: {df_encoded.shape}")
df_encoded.head()
```

## ğŸ¯ BÆ°á»›c 4: Feature Selection

### 4.1 Correlation Analysis
```python
# Báº£n Ä‘á»“ nhiá»‡t tÆ°Æ¡ng quan
plt.figure(figsize=(20, 16))
correlation_matrix = df_encoded.corr()
sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', center=0)
plt.title('Ma tráº­n tÆ°Æ¡ng quan Ä‘áº·c trÆ°ng')
plt.show()

# TÆ°Æ¡ng quan cao nháº¥t vá»›i má»¥c tiÃªu
target_corr = correlation_matrix['Churn'].abs().sort_values(ascending=False)
print("ğŸ¯ TOP 15 Äáº¶C TRÆ¯NG TÆ¯Æ NG QUAN Vá»šI CHURN:")
print(target_corr.head(15))
```

### 4.2 Feature Selection Methods
```python
# Chuáº©n bá»‹ dá»¯ liá»‡u
X = df_encoded.drop('Churn', axis=1)
y = df_encoded['Churn']

print(f"KÃ­ch thÆ°á»›c Ä‘áº·c trÆ°ng: {X.shape}")
print(f"PhÃ¢n bá»‘ target: {y.value_counts().to_dict()}")

# PhÆ°Æ¡ng phÃ¡p 1: SelectKBest
def select_features_univariate(X, y, k=10):
    selector = SelectKBest(score_func=f_classif, k=k)
    X_selected = selector.fit_transform(X, y)
    selected_features = X.columns[selector.get_support()].tolist()
    scores = selector.scores_
    
    feature_scores = pd.DataFrame({
        'feature': X.columns,
        'score': scores
    }).sort_values('score', ascending=False)
    
    return selected_features, feature_scores

# PhÆ°Æ¡ng phÃ¡p 2: Random Forest Feature Importance
def select_features_rf(X, y, k=10):
    rf = RandomForestClassifier(n_estimators=100, random_state=42)
    rf.fit(X, y)
    
    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': rf.feature_importances_
    }).sort_values('importance', ascending=False)
    
    selected_features = feature_importance.head(k)['feature'].tolist()
    return selected_features, feature_importance

# Ãp dá»¥ng lá»±a chá»n Ä‘áº·c trÆ°ng
for k in [5, 10, 15]:
    print(f"\nğŸ” TOP {k} Äáº¶C TRÆ¯NG:")
    
    # Lá»±a chá»n Ä‘Æ¡n biáº¿n
    features_uni, scores_uni = select_features_univariate(X, y, k)
    print(f"PhÆ°Æ¡ng phÃ¡p Univariate: {features_uni}")
    
    # Random Forest
    features_rf, scores_rf = select_features_rf(X, y, k)
    print(f"Random Forest: {features_rf}")
```

## ğŸ¤– BÆ°á»›c 5: Model Training vÃ  Evaluation

### 5.1 Data Splitting vÃ  Scaling
```python
# Chia dá»¯ liá»‡u
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Chuáº©n hÃ³a
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"Táº­p huáº¥n luyá»‡n: {X_train_scaled.shape}")
print(f"Táº­p kiá»ƒm tra: {X_test_scaled.shape}")
print(f"PhÃ¢n bá»‘ target trong táº­p train: {y_train.value_counts(normalize=True).round(3).to_dict()}")
```

### 5.2 Model Training
```python
# Import cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t cho thuáº­t toÃ¡n
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.model_selection import cross_val_score

class MutilModel:
    def __init__(self):
        self.models = {
            'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),
            'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
            'SVM': SVC(probability=True, random_state=42),
            'KNN': KNeighborsClassifier(n_neighbors=5),
            'NeuralNetwork': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)
        }

    def train(self, X_train, y_train):
        for model in self.models.values():
            model.fit(X_train, y_train)

    def evaluation(self, X_test, y_test, X_train=None, y_train=None, cv=5):
        report = {}

        for name, model in self.models.items():
            y_pred = model.predict(X_test)

            # TÃ­nh AUC náº¿u cÃ³ há»— trá»£ predict_proba
            if hasattr(model, "predict_proba"):
                y_proba = model.predict_proba(X_test)[:, 1]
                auc = roc_auc_score(y_test, y_proba)
            else:
                auc = None

            # Cross-validation
            if X_train is not None and y_train is not None:
                cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')
                cv_mean = cv_scores.mean()
                cv_std = cv_scores.std()
            else:
                cv_mean = None
                cv_std = None

            report[name] = {
                'accuracy': accuracy_score(y_test, y_pred),
                'precision': precision_score(y_test, y_pred),
                'recall': recall_score(y_test, y_pred),
                'f1_score': f1_score(y_test, y_pred),
                'auc': auc,
                'cv_mean': cv_mean,
                'cv_std': cv_std
            }

        return report

# Khá»Ÿi táº¡o vÃ  sá»­ dá»¥ng MutilModel
print("ğŸ¤– KHá»I Táº O VÃ€ HUáº¤N LUYá»†N CÃC MÃ” HÃŒNH:")
multi_model = MutilModel()

print("ğŸ“‹ CÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c sá»­ dá»¥ng:")
for name in multi_model.models.keys():
    print(f"   - {name}")

# Huáº¥n luyá»‡n táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh
print(f"\nğŸ”„ Äang huáº¥n luyá»‡n {len(multi_model.models)} mÃ´ hÃ¬nh...")
multi_model.train(X_train_scaled, y_train)
print("âœ… HoÃ n thÃ nh huáº¥n luyá»‡n!")

# ÄÃ¡nh giÃ¡ cÃ¡c mÃ´ hÃ¬nh
print(f"\nğŸ“Š ÄÃNH GIÃ HIá»†U SUáº¤T:")
results = multi_model.evaluation(X_test_scaled, y_test, X_train_scaled, y_train)

# In káº¿t quáº£ chi tiáº¿t
for name, metrics in results.items():
    print(f"\nğŸ”¹ {name}:")
    print(f"   Accuracy:  {metrics['accuracy']:.4f}")
    print(f"   Precision: {metrics['precision']:.4f}")
    print(f"   Recall:    {metrics['recall']:.4f}")
    print(f"   F1-Score:  {metrics['f1_score']:.4f}")
    if metrics['auc'] is not None:
        print(f"   AUC:       {metrics['auc']:.4f}")
    if metrics['cv_mean'] is not None:
        print(f"   CV Score:  {metrics['cv_mean']:.4f} (Â±{metrics['cv_std']:.4f})")
```

### 5.2.1 Linear Regression Analysis (Lecture 3)
```python
# Import thÃªm cho Linear Regression vÃ  Clustering
from sklearn.linear_model import LinearRegression
from sklearn.cluster import KMeans
from sklearn.metrics import r2_score, mean_squared_error, silhouette_score

# Lecture 3: Há»“i quy tuyáº¿n tÃ­nh - PhÃ¢n tÃ­ch má»‘i quan há»‡
print("\nğŸ“ˆ PHÃ‚N TÃCH Há»’I QUY TUYáº¾N TÃNH:")

# Há»“i quy MonthlyCharges dá»±a trÃªn tenure
lr_monthly = LinearRegression()
X_tenure = df_encoded[['tenure']].values
y_monthly = df_encoded['MonthlyCharges'].values

lr_monthly.fit(X_tenure, y_monthly)
y_pred_monthly = lr_monthly.predict(X_tenure)

r2_monthly = r2_score(y_monthly, y_pred_monthly)
mse_monthly = mean_squared_error(y_monthly, y_pred_monthly)

print(f"Há»“i quy MonthlyCharges ~ tenure:")
print(f"RÂ²: {r2_monthly:.4f}")
print(f"MSE: {mse_monthly:.4f}")
print(f"Há»‡ sá»‘ há»“i quy: {lr_monthly.coef_[0]:.4f}")
print(f"Intercept: {lr_monthly.intercept_:.4f}")

# Váº½ biá»ƒu Ä‘á»“ há»“i quy
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.scatter(X_tenure, y_monthly, alpha=0.5, label='Dá»¯ liá»‡u thá»±c')
plt.plot(X_tenure, y_pred_monthly, 'r-', label='ÄÆ°á»ng há»“i quy')
plt.xlabel('Tenure (thÃ¡ng)')
plt.ylabel('Monthly Charges')
plt.title(f'Há»“i quy tuyáº¿n tÃ­nh\nRÂ² = {r2_monthly:.4f}')
plt.legend()

# Há»“i quy TotalCharges dá»±a trÃªn tenure + MonthlyCharges
lr_total = LinearRegression()
X_multi = df_encoded[['tenure', 'MonthlyCharges']].values
y_total = df_encoded['TotalCharges'].values

lr_total.fit(X_multi, y_total)
y_pred_total = lr_total.predict(X_multi)
r2_total = r2_score(y_total, y_pred_total)

print(f"\nHá»“i quy TotalCharges ~ tenure + MonthlyCharges:")
print(f"RÂ²: {r2_total:.4f}")
print(f"Há»‡ sá»‘ há»“i quy: {lr_total.coef_}")

plt.subplot(1, 2, 2)
plt.scatter(y_total, y_pred_total, alpha=0.5)
plt.plot([y_total.min(), y_total.max()], [y_total.min(), y_total.max()], 'r--')
plt.xlabel('TotalCharges thá»±c táº¿')
plt.ylabel('TotalCharges dá»± Ä‘oÃ¡n')
plt.title(f'Dá»± Ä‘oÃ¡n vs Thá»±c táº¿\nRÂ² = {r2_total:.4f}')

plt.tight_layout()
plt.show()
```

### 5.2.2 Clustering Analysis (Lecture 4+5)
```python
# Lecture 4+5: PhÃ¢n cá»¥m khÃ¡ch hÃ ng
print("\nğŸ¯ PHÃ‚N TÃCH PHÃ‚N Cá»¤M KHÃCH HÃ€NG:")

# Chuáº©n bá»‹ dá»¯ liá»‡u cho clustering (chá»‰ dÃ¹ng numerical features)
cluster_features = ['tenure', 'MonthlyCharges', 'TotalCharges']
X_cluster = df_encoded[cluster_features].values
X_cluster_scaled = StandardScaler().fit_transform(X_cluster)

# TÃ¬m sá»‘ cluster tá»‘i Æ°u báº±ng Elbow method
inertias = []
silhouette_scores = []
K_range = range(2, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_cluster_scaled)
    inertias.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(X_cluster_scaled, kmeans.labels_))

# Váº½ biá»ƒu Ä‘á»“ Elbow vÃ  Silhouette
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# Elbow Method
axes[0].plot(K_range, inertias, 'bo-')
axes[0].set_xlabel('Sá»‘ cá»¥m (K)')
axes[0].set_ylabel('Inertia')
axes[0].set_title('PhÆ°Æ¡ng phÃ¡p Elbow')
axes[0].grid(True)

# Silhouette Score
axes[1].plot(K_range, silhouette_scores, 'ro-')
axes[1].set_xlabel('Sá»‘ cá»¥m (K)')
axes[1].set_ylabel('Silhouette Score')
axes[1].set_title('Äiá»ƒm Silhouette')
axes[1].grid(True)

# Chá»n K tá»‘i Æ°u (K=3 hoáº·c K cÃ³ silhouette score cao nháº¥t)
optimal_k = K_range[np.argmax(silhouette_scores)]
print(f"Sá»‘ cá»¥m tá»‘i Æ°u: {optimal_k} (Silhouette Score: {max(silhouette_scores):.4f})")

# Thá»±c hiá»‡n clustering vá»›i K tá»‘i Æ°u
kmeans_optimal = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
cluster_labels = kmeans_optimal.fit_predict(X_cluster_scaled)

# ThÃªm cluster labels vÃ o dataframe
df_encoded['Cluster'] = cluster_labels

# PhÃ¢n tÃ­ch clusters
axes[2].scatter(df_encoded['tenure'], df_encoded['MonthlyCharges'], 
               c=cluster_labels, cmap='viridis', alpha=0.6)
axes[2].set_xlabel('Tenure')
axes[2].set_ylabel('Monthly Charges')
axes[2].set_title(f'PhÃ¢n cá»¥m khÃ¡ch hÃ ng (K={optimal_k})')

plt.tight_layout()
plt.show()

# PhÃ¢n tÃ­ch Ä‘áº·c Ä‘iá»ƒm tá»«ng cluster
print("\nğŸ“Š Äáº¶C ÄIá»‚M CÃC CLUSTER:")
for i in range(optimal_k):
    cluster_data = df_encoded[df_encoded['Cluster'] == i]
    churn_rate = cluster_data['Churn'].mean()
    
    print(f"\nğŸ”¸ Cluster {i} ({len(cluster_data)} khÃ¡ch hÃ ng):")
    print(f"   Tá»· lá»‡ Churn: {churn_rate:.2%}")
    print(f"   Tenure trung bÃ¬nh: {cluster_data['tenure'].mean():.1f} thÃ¡ng")
    print(f"   MonthlyCharges trung bÃ¬nh: ${cluster_data['MonthlyCharges'].mean():.2f}")
    print(f"   TotalCharges trung bÃ¬nh: ${cluster_data['TotalCharges'].mean():.2f}")
```

### 5.3 Results Visualization
```python
# Chuyá»ƒn Ä‘á»•i káº¿t quáº£ sang DataFrame Ä‘á»ƒ dá»… phÃ¢n tÃ­ch
results_df = pd.DataFrame(results).T

print("ğŸ“Š Báº¢NG SO SÃNH CHI TIáº¾T CÃC MÃ” HÃŒNH:")
print(results_df.round(4))

# TÃ¬m mÃ´ hÃ¬nh tá»‘t nháº¥t theo tá»«ng metric
print(f"\nğŸ† MÃ” HÃŒNH Tá»T NHáº¤T THEO Tá»ªNG METRIC:")
print(f"   Accuracy:  {results_df['accuracy'].idxmax()} ({results_df['accuracy'].max():.4f})")
print(f"   Precision: {results_df['precision'].idxmax()} ({results_df['precision'].max():.4f})")
print(f"   Recall:    {results_df['recall'].idxmax()} ({results_df['recall'].max():.4f})")
print(f"   F1-Score:  {results_df['f1_score'].idxmax()} ({results_df['f1_score'].max():.4f})")

# Biá»ƒu Ä‘á»“ so sÃ¡nh hiá»‡u suáº¥t
fig, axes = plt.subplots(2, 3, figsize=(18, 12))

# So sÃ¡nh Accuracy
results_df['accuracy'].plot(kind='bar', ax=axes[0,0], color='skyblue')
axes[0,0].set_title('So sÃ¡nh Accuracy')
axes[0,0].set_ylabel('Accuracy')
axes[0,0].tick_params(axis='x', rotation=45)
axes[0,0].set_ylim([results_df['accuracy'].min() - 0.05, 1.0])

# So sÃ¡nh Precision
results_df['precision'].plot(kind='bar', ax=axes[0,1], color='lightgreen')
axes[0,1].set_title('So sÃ¡nh Precision')
axes[0,1].set_ylabel('Precision')
axes[0,1].tick_params(axis='x', rotation=45)
axes[0,1].set_ylim([results_df['precision'].min() - 0.05, 1.0])

# So sÃ¡nh Recall
results_df['recall'].plot(kind='bar', ax=axes[0,2], color='lightcoral')
axes[0,2].set_title('So sÃ¡nh Recall')
axes[0,2].set_ylabel('Recall')
axes[0,2].tick_params(axis='x', rotation=45)
axes[0,2].set_ylim([results_df['recall'].min() - 0.05, 1.0])

# So sÃ¡nh F1-Score
results_df['f1_score'].plot(kind='bar', ax=axes[1,0], color='gold')
axes[1,0].set_title('So sÃ¡nh F1-Score')
axes[1,0].set_ylabel('F1-Score')
axes[1,0].tick_params(axis='x', rotation=45)
axes[1,0].set_ylim([results_df['f1_score'].min() - 0.05, 1.0])

# So sÃ¡nh AUC (chá»‰ cho models cÃ³ AUC)
auc_data = results_df[results_df['auc'].notna()]['auc']
if len(auc_data) > 0:
    auc_data.plot(kind='bar', ax=axes[1,1], color='plum')
    axes[1,1].set_title('So sÃ¡nh AUC Score')
    axes[1,1].set_ylabel('AUC')
    axes[1,1].tick_params(axis='x', rotation=45)
    axes[1,1].set_ylim([auc_data.min() - 0.05, 1.0])

# Cross Validation Scores vá»›i error bars
cv_data = results_df[results_df['cv_mean'].notna()]
if len(cv_data) > 0:
    cv_means = cv_data['cv_mean']
    cv_stds = cv_data['cv_std']
    x_pos = range(len(cv_means))
    axes[1,2].bar(x_pos, cv_means, yerr=cv_stds, capsize=5, color='orange', alpha=0.7)
    axes[1,2].set_title('Cross Validation Scores')
    axes[1,2].set_ylabel('CV Accuracy')
    axes[1,2].set_xticks(x_pos)
    axes[1,2].set_xticklabels(cv_means.index, rotation=45)

plt.tight_layout()
plt.show()

# Ma tráº­n nháº§m láº«n cho top 3 models theo accuracy
print(f"\nğŸ¯ MA TRáº¬N NHáº¦M LáºªN CHO TOP 3 MÃ” HÃŒNH:")
top_3_models = results_df.nlargest(3, 'accuracy').index

fig, axes = plt.subplots(1, min(3, len(top_3_models)), figsize=(15, 4))
if len(top_3_models) == 1:
    axes = [axes]

for i, model_name in enumerate(top_3_models[:3]):
    # Láº¥y predictions tá»« model
    model = multi_model.models[model_name]
    y_pred = model.predict(X_test_scaled)
    
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', ax=axes[i], cmap='Blues')
    axes[i].set_title(f'{model_name}\nAccuracy: {results[model_name]["accuracy"]:.3f}')
    axes[i].set_xlabel('Dá»± Ä‘oÃ¡n')
    axes[i].set_ylabel('Thá»±c táº¿')

plt.tight_layout()
plt.show()

# Biá»ƒu Ä‘á»“ radar cho so sÃ¡nh tá»•ng quan
import numpy as np

def create_radar_chart(models_data, metrics=['accuracy', 'precision', 'recall', 'f1_score']):
    fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(projection='polar'))
    
    # Sá»‘ lÆ°á»£ng metrics
    num_metrics = len(metrics)
    angles = np.linspace(0, 2 * np.pi, num_metrics, endpoint=False).tolist()
    angles += angles[:1]  # ÄÃ³ng vÃ²ng trÃ²n
    
    # Váº½ cho tá»«ng model
    colors = ['red', 'blue', 'green', 'orange', 'purple']
    for i, (model_name, data) in enumerate(models_data.items()):
        values = [data[metric] for metric in metrics]
        values += values[:1]  # ÄÃ³ng vÃ²ng trÃ²n
        
        ax.plot(angles, values, 'o-', linewidth=2, label=model_name, color=colors[i % len(colors)])
        ax.fill(angles, values, alpha=0.25, color=colors[i % len(colors)])
    
    # Thiáº¿t láº­p labels
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels([metric.title() for metric in metrics])
    ax.set_ylim(0, 1)
    ax.set_title('So sÃ¡nh tá»•ng quan cÃ¡c mÃ´ hÃ¬nh', size=16, y=1.1)
    ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))
    
    plt.tight_layout()
    plt.show()

# Táº¡o radar chart
print(f"\nğŸ“ˆ BIá»‚U Äá»’ RADAR SO SÃNH Tá»”NG QUAN:")
create_radar_chart(results)
```

### 5.4 Market Basket Analysis (Lecture 12)
```python
# Lecture 12: Khai phÃ¡ táº­p má»¥c thÆ°á»ng xuyÃªn vÃ  cÃ¡c luáº­t káº¿t há»£p
print("\nğŸ›’ PHÃ‚N TÃCH Táº¬P Má»¤C THÆ¯á»œNG XUYÃŠN (MARKET BASKET ANALYSIS):")

# CÃ i Ä‘áº·t thÆ° viá»‡n mlxtend náº¿u chÆ°a cÃ³
try:
    from mlxtend.frequent_patterns import apriori, association_rules
    from mlxtend.preprocessing import TransactionEncoder
except ImportError:
    print("CÃ i Ä‘áº·t mlxtend...")
    import subprocess
    subprocess.run(["pip", "install", "mlxtend"], check=True)
    from mlxtend.frequent_patterns import apriori, association_rules
    from mlxtend.preprocessing import TransactionEncoder

# Táº¡o dá»¯ liá»‡u giao dá»‹ch tá»« cÃ¡c dá»‹ch vá»¥
service_cols = ['PhoneService', 'MultipleLines', 'InternetService_DSL', 'InternetService_Fiber optic',
               'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 
               'StreamingTV', 'StreamingMovies']

# Lá»c cÃ¡c cá»™t dá»‹ch vá»¥ cÃ³ trong dá»¯ liá»‡u
available_service_cols = [col for col in service_cols if col in df_encoded.columns]
print(f"CÃ¡c dá»‹ch vá»¥ phÃ¢n tÃ­ch: {available_service_cols}")

# Táº¡o transactions (chá»‰ láº¥y cÃ¡c dá»‹ch vá»¥ Ä‘Æ°á»£c sá»­ dá»¥ng = 1)
transactions = []
for _, row in df_encoded.iterrows():
    transaction = []
    for col in available_service_cols:
        if col in df_encoded.columns and row[col] == 1:
            transaction.append(col)
    
    # ThÃªm thÃ´ng tin churn
    if row['Churn'] == 1:
        transaction.append('Churn_Yes')
    else:
        transaction.append('Churn_No')
    
    # ThÃªm thÃ´ng tin contract
    if 'Contract_One year' in df_encoded.columns and row['Contract_One year'] == 1:
        transaction.append('Contract_OneYear')
    elif 'Contract_Two year' in df_encoded.columns and row['Contract_Two year'] == 1:
        transaction.append('Contract_TwoYear')
    else:
        transaction.append('Contract_MonthToMonth')
    
    transactions.append(transaction)

# Chuyá»ƒn Ä‘á»•i thÃ nh format cho apriori
te = TransactionEncoder()
te_ary = te.fit(transactions).transform(transactions)
df_transactions = pd.DataFrame(te_ary, columns=te.columns_)

print(f"Sá»‘ lÆ°á»£ng transactions: {len(df_transactions)}")
print(f"Sá»‘ lÆ°á»£ng items: {len(te.columns_)}")
print(f"Items: {list(te.columns_)}")

# TÃ¬m frequent itemsets
frequent_itemsets = apriori(df_transactions, min_support=0.1, use_colnames=True)
frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))

print(f"\nğŸ“Š Sá» LÆ¯á»¢NG FREQUENT ITEMSETS:")
itemset_counts = frequent_itemsets['length'].value_counts().sort_index()
print(itemset_counts)

# Hiá»ƒn thá»‹ top frequent itemsets
print(f"\nğŸ” TOP 10 FREQUENT ITEMSETS:")
top_itemsets = frequent_itemsets.nlargest(10, 'support')
for idx, row in top_itemsets.iterrows():
    items = ', '.join(list(row['itemsets']))
    print(f"Support: {row['support']:.3f} | Items: {items}")

# Táº¡o association rules
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1.0)

if len(rules) > 0:
    # Sáº¯p xáº¿p theo lift
    rules_sorted = rules.sort_values('lift', ascending=False)
    
    print(f"\nğŸ”— TOP 10 ASSOCIATION RULES (sáº¯p xáº¿p theo Lift):")
    for idx, rule in rules_sorted.head(10).iterrows():
        antecedent = ', '.join(list(rule['antecedents']))
        consequent = ', '.join(list(rule['consequents']))
        print(f"Rule: {antecedent} â†’ {consequent}")
        print(f"   Support: {rule['support']:.3f}, Confidence: {rule['confidence']:.3f}, Lift: {rule['lift']:.3f}")
        print()
    
    # PhÃ¢n tÃ­ch rules liÃªn quan Ä‘áº¿n Churn
    churn_rules = rules_sorted[rules_sorted['consequents'].astype(str).str.contains('Churn')]
    
    if len(churn_rules) > 0:
        print(f"\nâš ï¸  TOP 5 RULES DáºªN Äáº¾N CHURN:")
        for idx, rule in churn_rules.head(5).iterrows():
            antecedent = ', '.join(list(rule['antecedents']))
            consequent = ', '.join(list(rule['consequents']))
            print(f"Rule: {antecedent} â†’ {consequent}")
            print(f"   Confidence: {rule['confidence']:.3f}, Lift: {rule['lift']:.3f}")
            print()
    
    # Visualization
    plt.figure(figsize=(15, 5))
    
    plt.subplot(1, 3, 1)
    plt.scatter(rules['support'], rules['confidence'], alpha=0.6, c=rules['lift'])
    plt.xlabel('Support')
    plt.ylabel('Confidence')
    plt.title('Support vs Confidence')
    plt.colorbar(label='Lift')
    
    plt.subplot(1, 3, 2)
    plt.hist(rules['lift'], bins=20, alpha=0.7, color='skyblue')
    plt.xlabel('Lift')
    plt.ylabel('Frequency')
    plt.title('PhÃ¢n bá»‘ Lift')
    
    plt.subplot(1, 3, 3)
    itemset_counts.plot(kind='bar', color='lightgreen')
    plt.xlabel('Äá»™ dÃ i Itemset')
    plt.ylabel('Sá»‘ lÆ°á»£ng')
    plt.title('PhÃ¢n bá»‘ Ä‘á»™ dÃ i Itemset')
    
    plt.tight_layout()
    plt.show()
else:
    print("KhÃ´ng tÃ¬m tháº¥y association rules vá»›i threshold hiá»‡n táº¡i.")
```

## ğŸ¯ BÆ°á»›c 6: Feature Selection Performance

### 6.1 Test vá»›i Different Feature Sets
```python
# Kiá»ƒm tra hiá»‡u suáº¥t vá»›i cÃ¡c bá»™ Ä‘áº·c trÆ°ng khÃ¡c nhau
feature_sets = {
    'Top 5 Univariate': select_features_univariate(X, y, 5)[0],
    'Top 10 Univariate': select_features_univariate(X, y, 10)[0],
    'Top 15 Univariate': select_features_univariate(X, y, 15)[0],
    'Top 5 RF': select_features_rf(X, y, 5)[0],
    'Top 10 RF': select_features_rf(X, y, 10)[0],
    'Top 15 RF': select_features_rf(X, y, 15)[0],
}

# Hiá»‡u suáº¥t vá»›i cÃ¡c táº­p con Ä‘áº·c trÆ°ng
subset_results = {}

for subset_name, features in feature_sets.items():
    print(f"\nğŸ” Äang kiá»ƒm tra {subset_name} ({len(features)} Ä‘áº·c trÆ°ng)...")
    
    # Táº¡o táº­p con dá»¯ liá»‡u
    X_subset = X[features]
    X_train_sub, X_test_sub, y_train_sub, y_test_sub = train_test_split(
        X_subset, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # Chuáº©n hÃ³a
    X_train_sub_scaled = scaler.fit_transform(X_train_sub)
    X_test_sub_scaled = scaler.transform(X_test_sub)
    
    # Huáº¥n luyá»‡n mÃ´ hÃ¬nh tá»‘t nháº¥t (Random Forest)
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train_sub_scaled, y_train_sub)
    
    # ÄÃ¡nh giÃ¡
    y_pred_sub = model.predict(X_test_sub_scaled)
    accuracy_sub = accuracy_score(y_test_sub, y_pred_sub)
    
    subset_results[subset_name] = {
        'n_features': len(features),
        'accuracy': accuracy_sub,
        'features': features
    }
    
    print(f"   Äá»™ chÃ­nh xÃ¡c: {accuracy_sub:.4f} vá»›i {len(features)} Ä‘áº·c trÆ°ng")

# Biá»ƒu Ä‘á»“ káº¿t quáº£ lá»±a chá»n Ä‘áº·c trÆ°ng
subset_df = pd.DataFrame(subset_results).T
subset_df = subset_df.sort_values('accuracy', ascending=False)

plt.figure(figsize=(12, 6))
bars = plt.bar(range(len(subset_df)), subset_df['accuracy'], color='lightcoral')
plt.xlabel('PhÆ°Æ¡ng phÃ¡p lá»±a chá»n Ä‘áº·c trÆ°ng')
plt.ylabel('Äá»™ chÃ­nh xÃ¡c')
plt.title('Hiá»‡u suáº¥t theo phÆ°Æ¡ng phÃ¡p lá»±a chá»n Ä‘áº·c trÆ°ng')
plt.xticks(range(len(subset_df)), subset_df.index, rotation=45)

# ThÃªm nhÃ£n giÃ¡ trá»‹ trÃªn cá»™t
for i, bar in enumerate(bars):
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height + 0.005,
             f'{height:.3f}\n({subset_df.iloc[i]["n_features"]} Ä‘áº·c trÆ°ng)',
             ha='center', va='bottom')

plt.tight_layout()
plt.show()

print("\nğŸ† Káº¾T QUáº¢ CHá»ŒN Äáº¶C TRÆ¯NG Tá»T NHáº¤T:")
print(subset_df.sort_values('accuracy', ascending=False))
```

## ğŸ’¾ BÆ°á»›c 7: Save Results

### 7.1 Export Results
```python
# LÆ°u káº¿t quáº£ ra CSV
results_df.to_csv('model_comparison_results.csv')
subset_df.to_csv('feature_selection_results.csv')

# Táº£i xuá»‘ng cÃ¡c file
from google.colab import files
files.download('model_comparison_results.csv')
files.download('feature_selection_results.csv')

print("âœ… Káº¿t quáº£ Ä‘Ã£ Ä‘Æ°á»£c lÆ°u vÃ  táº£i xuá»‘ng!")
```

## ğŸ‰ HoÃ n thÃ nh!

### ğŸ“Š Summary
```python
print("ğŸ¯ TÃ“M Táº®T Dá»° ÃN")
print("=" * 50)
print(f"ğŸ“Š Dá»¯ liá»‡u: {df.shape[0]} khÃ¡ch hÃ ng, {df.shape[1]} Ä‘áº·c trÆ°ng")
print(f"ğŸ¯ Má»¥c tiÃªu: Customer Churn ({(y.sum() / len(y) * 100):.1f}% tá»· lá»‡ churn)")
print(f"ğŸ”§ Äáº·c trÆ°ng sau tiá»n xá»­ lÃ½: {X.shape[1]}")
print(f"ğŸ† MÃ´ hÃ¬nh tá»‘t nháº¥t: {results_df['accuracy'].idxmax()} ({results_df['accuracy'].max():.3f})")
print(f"ğŸ¯ Bá»™ Ä‘áº·c trÆ°ng tá»‘t nháº¥t: {subset_df.index[0]} ({subset_df.iloc[0]['accuracy']:.3f})")
print("\nâœ… PhÃ¢n tÃ­ch hoÃ n thÃ nh thÃ nh cÃ´ng!")
```

## ğŸš€ Tips cho Google Colab

### Tá»‘i Æ°u Performance:
- Sá»­ dá»¥ng GPU: **Runtime > Change runtime type > GPU**
- Disconnect sau khi xong: **Runtime > Disconnect**
- Restart náº¿u memory Ä‘áº§y: **Runtime > Restart runtime**

### LÆ°u trá»¯:
- Mount Google Drive Ä‘á»ƒ lÆ°u permanent
- Download results quan trá»ng vá» mÃ¡y
- Copy notebook vÃ o Drive Ä‘á»ƒ backup

### Troubleshooting:
- Náº¿u bá»‹ disconnect: Re-run tá»« Ä‘áº§u
- Náº¿u thiáº¿u package: `!pip install package_name`
- Náº¿u lá»—i memory: Giáº£m data size hoáº·c dÃ¹ng sampling

---

**ğŸ‰ ChÃºc báº¡n thÃ nh cÃ´ng vá»›i project Machine Learning!** 